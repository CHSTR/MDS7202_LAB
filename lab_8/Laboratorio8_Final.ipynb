{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "97c30de09ab74ff8a8ead49b42589d52",
    "deepnote_cell_height": 156.390625,
    "deepnote_cell_type": "markdown",
    "id": "XUZ1dFPHzAHl"
   },
   "source": [
    "<h1><center>Laboratorio 8: 쯉uperh칠roe o Villano? 游붲</center></h1>\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "44f0e095cf894684b9513510e9517584",
    "deepnote_cell_type": "markdown",
    "id": "UD8X1uhGzAHq"
   },
   "source": [
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Mat칤as Rojas y Mauricio Araneda\n",
    "- Auxiliar: Ignacio Meza D.\n",
    "- Ayudante: Rodrigo Guerra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "630dba7238124007a872bee8e5ac1068",
    "deepnote_cell_height": 171.78125,
    "deepnote_cell_type": "markdown",
    "id": "tXflExjqzAHr"
   },
   "source": [
    "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados\n",
    "\n",
    "- Nombre de alumno 1: Diego Cortez\n",
    "- Nombre de alumno 2: Christopher Stears\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b3307d6b91384799b5ffb79d87b50821",
    "deepnote_cell_height": 63,
    "deepnote_cell_type": "markdown",
    "id": "AD-V0bbZzAHr"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [https://github.com/CHSTR/MDS7202_LAB](https://github.com/CHSTR/MDS7202_LAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "29929e237c7e4bd58e32e7014be15f5d",
    "deepnote_cell_type": "markdown",
    "id": "6uBLPj1PzAHs"
   },
   "source": [
    "## Temas a tratar\n",
    "\n",
    "- Codificaci칩n de texto usando Bag of Words.\n",
    "- B칰squeda del modelo 칩ptimo de clasificaci칩n usando `GridSearch`\n",
    "- Uso de pipelines.\n",
    "\n",
    "## Reglas:\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- **Ausentes** deber치n realizar la actividad solos. \n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n",
    "- Prohibidas las copias. \n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### Objetivos principales del laboratorio\n",
    "\n",
    "- Obtener caracteristicas a partir de texto usando `CountVectorizer`.\n",
    "- Fijar un pipeline con un modelo base que luego se ir치 optimizando.\n",
    "- Comprender como realizar una b칰squeda de grilla sobre un conjunto de clasificadores e hiperpar치metros usando `GridSearch`.\n",
    "\n",
    "El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f5ade8a9983a447fb8a70a3f15daee03",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "id": "MhISwri4zAHy"
   },
   "source": [
    "#Importamos librerias utiles 游땾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T00:08:16.884674Z",
     "start_time": "2021-03-29T00:08:16.349846Z"
    },
    "cell_id": "6b4998dbdd2b486f9a27fc4040ca9c26",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_height": 1509.1875,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "executionInfo": {
     "elapsed": 9106,
     "status": "ok",
     "timestamp": 1625497725070,
     "user": {
      "displayName": "IGNACIO ALEJANDRO MEZA",
      "photoUrl": "",
      "userId": "17011121633069169364"
     },
     "user_tz": 240
    },
    "execution_millis": 36497,
    "execution_start": 1637348694866,
    "id": "uyc33dKdzAHy",
    "outputId": "14d5da48-5dae-4ce4-a8ba-b826ea91a126",
    "source_hash": "7ce9748b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (5.11.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: umap-learn in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from umap-learn) (4.64.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from umap-learn) (0.5.8)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from umap-learn) (0.56.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from umap-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from umap-learn) (1.9.1)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from numba>=0.49->umap-learn) (63.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from tqdm->umap-learn) (0.4.5)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\envs\\datamining\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librer칤a Core del lab.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Pre-procesamiento\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Clasifaci칩n\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "# Metricas de evaluaci칩n\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Librer칤a para plotear\n",
    "!pip install --upgrade plotly\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Proyecciones en baja dimensionalidad: UMAP\n",
    "!pip install umap-learn\n",
    "\n",
    "# Librer칤a para NLP\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "17fd1c4a7c1045a685dff360277240e7",
    "deepnote_cell_height": 82,
    "deepnote_cell_type": "markdown",
    "id": "xpOTbQcxbSiy"
   },
   "source": [
    "# 1. 쯈uien es Bat Cow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "49b52b8931f04a2fa4287082eb3da154",
    "deepnote_cell_height": 347.8125,
    "deepnote_cell_type": "markdown",
    "id": "3Q93vbNS25bM"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.imgur.com/D9f1RHy.jpg\" width=\"350\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "98dbaf188dec4147b2b7e981fc64c61f",
    "deepnote_cell_height": 503.03125,
    "deepnote_cell_type": "markdown",
    "id": "jnmZfFpxTTYX"
   },
   "source": [
    "En vez de estar desarrollando las evaluaciones correspondientes a su curso, su profesor de catedra y su auxiliar discuten acerca la alineaci칩n (h칠roe o villano) del personaje de ficci칩n Bat-Cow. \n",
    "\n",
    "El cuerpo docente, no logra ponerse de acuerdo si el personaje es bueno, neutral o malo: el auxiliar plantea que Bat-cow posee una siniestra mirada, intrigante pero com칰n caracter칤stica de los personajes malvados. \n",
    "Por otra parte, extendiendo las ideas de Rousseau, el profesor plantea que tal como los humanos no nacen malos, no existe motivo por el cual una vaca con superpoderes deba serlo.\n",
    "\n",
    "Sin embargo, ambos concuerdan que es dif칤cil estimar la alineaci칩n solo usando los atributos f칤sicos, por lo que creen el an치lisis debe ser complementado a칰n m치s antes de comunicarle los resultados a su estudiantado. Buscando m치s informaci칩n, ambos sujetos se percatan de la existencia de un excelente antecedente para estimar la alineaci칩n: la historia personal de cada superh칠roe o villano.\n",
    "\n",
    "Es por esto le solicitan que construya y optimice un clasificador basado en texto el cual analice la alineaci칩n de cada personaje basado en su historia personal.\n",
    "\n",
    "Para este laboratorio deben trabajar con los datos `df_comics.csv` y `comics_no_label.csv` subidos a u-cursos. El primero es un conjunto de datos que les servir치 para entrenar un modelo de clasificaci칩n, mientras que el segundo es un dataset con personajes de ficci칩n no etiquetados a predecir (s칤, aqu칤 est치 la misteriosa Batcow).\n",
    "\n",
    "Para comenzar cargue los dataset se침alados y visualice a trav칠s de un head los atributos que poseen cada uno de los dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "82557a3ea7744f42a77dc7f5d97629ab",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_height": 220.1875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1625497735673,
     "user": {
      "displayName": "IGNACIO ALEJANDRO MEZA",
      "photoUrl": "",
      "userId": "17011121633069169364"
     },
     "user_tz": 240
    },
    "execution_millis": 27,
    "execution_start": 1637348657022,
    "id": "Jqq-s010Iwl1",
    "outputId": "bc29f770-d066-4443-8cee-00e3db46c629",
    "source_hash": "c60dc4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignorando conexi칩n drive-colab\n"
     ]
    }
   ],
   "source": [
    "# Si usted est치 utilizando Colabolatory le puede ser 칰til este c칩digo para cargar los archivos.\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    path = 'Direcci칩n donde tiene los archivos en el Drive'\n",
    "except: \n",
    "    print('Ignorando conexi칩n drive-colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "c4b057f299d348fa839ac3c555241045",
    "deepnote_cell_height": 117,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1625499009249,
     "user": {
      "displayName": "IGNACIO ALEJANDRO MEZA",
      "photoUrl": "",
      "userId": "17011121633069169364"
     },
     "user_tz": 240
    },
    "execution_millis": 325,
    "execution_start": 1637348732856,
    "id": "bED3w3tDbSCf",
    "source_hash": "443d6e8"
   },
   "outputs": [],
   "source": [
    "df_comics = pd.read_csv('df_comics.csv', index_col = 0)\n",
    "df_comics_no_label = pd.read_csv('comics_no_label.csv', index_col = 0)\n",
    "df_comics = df_comics.dropna(subset=['history_text']) # eliminar ejemplos sin historia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "4c6367f4379042cc8515e0e86849acf3",
    "deepnote_cell_height": 323.1875,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     208.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 654,
    "execution_start": 1637348731943,
    "source_hash": "b986316d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>real_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>overall_score</th>\n",
       "      <th>history_text</th>\n",
       "      <th>powers_text</th>\n",
       "      <th>intelligence_score</th>\n",
       "      <th>strength_score</th>\n",
       "      <th>speed_score</th>\n",
       "      <th>durability_score</th>\n",
       "      <th>...</th>\n",
       "      <th>has_flight</th>\n",
       "      <th>has_accelerated_healing</th>\n",
       "      <th>has_weapons_master</th>\n",
       "      <th>has_intelligence</th>\n",
       "      <th>has_reflexes</th>\n",
       "      <th>has_super_speed</th>\n",
       "      <th>has_durability</th>\n",
       "      <th>has_stamina</th>\n",
       "      <th>has_agility</th>\n",
       "      <th>has_super_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-D Man</td>\n",
       "      <td>Delroy Garrett, Jr.</td>\n",
       "      <td>Delroy Garrett, Jr.</td>\n",
       "      <td>6</td>\n",
       "      <td>Delroy Garrett, Jr. grew up to become a track ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-Bomb</td>\n",
       "      <td>Richard Milhouse Jones</td>\n",
       "      <td>Richard Milhouse Jones</td>\n",
       "      <td>20</td>\n",
       "      <td>Richard \"Rick\" Jones was orphaned at a young ...</td>\n",
       "      <td>On rare occasions, and through unusual circu...</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aa</td>\n",
       "      <td>Aa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>Aa is one of the more passive members of the P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Cash</td>\n",
       "      <td>Aaron Cash</td>\n",
       "      <td>Aaron Cash</td>\n",
       "      <td>5</td>\n",
       "      <td>Aaron Cash is the head of security at Arkham A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aayla Secura</td>\n",
       "      <td>Aayla Secura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>ayla Secura was a Rutian Twi'lek Jedi Knight (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows 칑 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name               real_name               full_name overall_score  \\\n",
       "0       3-D Man     Delroy Garrett, Jr.     Delroy Garrett, Jr.             6   \n",
       "2        A-Bomb  Richard Milhouse Jones  Richard Milhouse Jones            20   \n",
       "3            Aa                      Aa                     NaN            12   \n",
       "4    Aaron Cash              Aaron Cash              Aaron Cash             5   \n",
       "5  Aayla Secura            Aayla Secura                     NaN             8   \n",
       "\n",
       "                                        history_text  \\\n",
       "0  Delroy Garrett, Jr. grew up to become a track ...   \n",
       "2   Richard \"Rick\" Jones was orphaned at a young ...   \n",
       "3  Aa is one of the more passive members of the P...   \n",
       "4  Aaron Cash is the head of security at Arkham A...   \n",
       "5  ayla Secura was a Rutian Twi'lek Jedi Knight (...   \n",
       "\n",
       "                                         powers_text  intelligence_score  \\\n",
       "0                                                NaN                  85   \n",
       "2    On rare occasions, and through unusual circu...                  80   \n",
       "3                                                NaN                  80   \n",
       "4                                                NaN                  80   \n",
       "5                                                NaN                  90   \n",
       "\n",
       "   strength_score  speed_score  durability_score  ...  has_flight  \\\n",
       "0              30           60                60  ...         0.0   \n",
       "2             100           80               100  ...         0.0   \n",
       "3              50           55                45  ...         0.0   \n",
       "4              10           25                40  ...         0.0   \n",
       "5              40           45                55  ...         0.0   \n",
       "\n",
       "   has_accelerated_healing has_weapons_master has_intelligence has_reflexes  \\\n",
       "0                      0.0                0.0              0.0          0.0   \n",
       "2                      1.0                0.0              0.0          1.0   \n",
       "3                      0.0                0.0              0.0          0.0   \n",
       "4                      0.0                1.0              0.0          0.0   \n",
       "5                      1.0                0.0              0.0          0.0   \n",
       "\n",
       "  has_super_speed has_durability has_stamina has_agility has_super_strength  \n",
       "0             1.0            0.0         0.0         0.0                1.0  \n",
       "2             1.0            1.0         1.0         1.0                1.0  \n",
       "3             0.0            0.0         0.0         0.0                0.0  \n",
       "4             0.0            0.0         0.0         0.0                0.0  \n",
       "5             0.0            0.0         0.0         1.0                0.0  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# queda a labor de su equipo hacer el an치lisis exploratorio\n",
    "df_comics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1285, 81)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2b5684743aec4d388edaa8f190652e9e",
    "deepnote_cell_height": 410,
    "deepnote_cell_type": "markdown",
    "id": "i4tFPrFA4_O5"
   },
   "source": [
    "## 1.1 Obtenci칩n de Features y Bag of Words\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media0.giphy.com/media/eIUpSyzwGp0YhAMTKr/200.gif\" width=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "39e5b3d761274681bdafab029d6ede31",
    "deepnote_cell_height": 561.859375,
    "deepnote_cell_type": "markdown",
    "id": "f_4NF0_V5XZ-"
   },
   "source": [
    "Primero que todo, deben obtener un vector de caracter칤sticas del atributo `history_text`, utilizando `Bag of Words`. En este atributo se presenta una breve descripci칩n de la historia de cada uno de los personajes de ficci칩n presentes en el dataset. \n",
    "\n",
    "Pero... antes de empezar, 쯈ue es `Bag of Words`?...\n",
    "\n",
    "`Bag of Words` es un modelo de conteo utilizado en Procesamiento de Lenguaje Natural (NLP) que tiene como objetivo generar una representaci칩n vectorial (vector de caracter칤sticas en nuestro cas) para cada documento a trav칠s del conteo de las palabras que contienen. \n",
    "\n",
    "La siguiente figura muestra un ejemplo de `Bag of Words` en acci칩n:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://user.oc-static.com/upload/2020/10/23/16034397439042_surfin%20bird%20bow.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "Como pueden ver, el modelo de `Bag of Words` no resulta tan complicado, 쯣ero c칩mo lo aplicamos en python?. \n",
    "\n",
    "Como podr치n darse cuenta del ejemplo anterior, para facilitar el conteo ser치 necesario transformar cada uno de los documentos en vectores, donde cada una de las posiciones posee un car치cter. Este proceso es conocido como **tokenizaci칩n** y lo podemos realizar de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "4225703f5ca94caeba76b5271167c8ec",
    "deepnote_cell_height": 227.375,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     40.375
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1637346921830,
    "source_hash": "57e4888a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'teacher', 'rocks', 'like', 'a', 'good', 'rock', '&', 'roll'],\n",
       " ['the', 'rock', 'is', 'the', 'best', 'actor', 'in', 'the', 'world']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The teacher rocks like a good rock & roll',\n",
    "             'the rock is the best actor in the world']\n",
    "\n",
    "\n",
    "docs_tokenizados = [word_tokenize(doc)  for doc in docs]\n",
    "docs_tokenizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38363e52900d4138aa13cfc88bb699dc",
    "deepnote_cell_height": 424.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Podemos mejorar un poco m치s el proceso de tokenizaci칩n agregando \n",
    "\n",
    "- Stemming:  Definimos Stemming como un algoritmo basado en reglas que transforma las palabras a una forma general. Un ejemplo de stemming, es el siguiente:\n",
    "- Eliminaci칩n de Stopwords: Eliminaci칩n de palabras muy frecuentes que entorpecen la clasificaci칩n (por ejemplo, el, la los, la, etc...)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://devopedia.org/images/article/218/8583.1569386710.png\" width=\"300\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "281adedd8d3e4785931934eadd57abfb",
    "deepnote_cell_height": 690.9375,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     59.5625
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 36,
    "execution_start": 1637346924545,
    "source_hash": "d7f59237",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['teacher', 'rock', 'like', 'good', 'rock', '&', 'roll'],\n",
       " ['rock', 'best', 'actor', 'world'],\n",
       " ['new', 'york', 'beauti', 'citi']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos algunas stopword que queremos que sean eliminadas\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Definimos un tokenizador con Stemming.\n",
    "# Fixed: Se definen las stopwords en la clase (m치s ordenado), se especifica el idioma\n",
    "# Fixed: Se aplica Stemming antes de eliminar stopwords, para no tener problemas con las may칰sculas\n",
    "class StemmerTokenizer:\n",
    "    def __init__(self, language):\n",
    "        self.ps = PorterStemmer()\n",
    "        self.stop_words = stopwords.words(language)\n",
    "    def __call__(self, doc):\n",
    "        doc_tok = word_tokenize(doc)\n",
    "        doc_tok = [self.ps.stem(t) for t in doc_tok]\n",
    "        return [t for t in doc_tok if t not in self.stop_words]\n",
    "\n",
    "# Inicializamos tokenizador\n",
    "tokenizador = StemmerTokenizer(\"english\")\n",
    "\n",
    "# Creamos algunos documentos\n",
    "docs = ['The teacher rocks like a good rock & roll',\n",
    "        'The rock is the best actor in the world',\n",
    "        'New York is a beautiful city']\n",
    "\n",
    "# Obtenemos el token del primer documento\n",
    "[tokenizador(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "3d0f1a2529964cedb72ccc62a8e818a5",
    "deepnote_cell_height": 192.5625,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     59.5625
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1637346927213,
    "source_hash": "2503a9b4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'teacher', 'rocks', 'like', 'a', 'good', 'rock', '&', 'roll'],\n",
       " ['The', 'rock', 'is', 'the', 'best', 'actor', 'in', 'the', 'world'],\n",
       " ['New', 'York', 'is', 'a', 'beautiful', 'city']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparaci칩n con el caso anterior\n",
    "docs_tokenizados = [word_tokenize(doc) for doc in docs]\n",
    "docs_tokenizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "937e18f6c6a747f7893b43a25d6aa65c",
    "deepnote_cell_height": 110.78125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Al Estilo Scikit\n",
    "\n",
    "Scikit implementa `bag of words` a trav칠s de la clase `CountVectorizer()` la cual contiene muchas opciones para mejorar la tokenizaci칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "c898d0dca5bd47cd80f4a8566cf5cc13",
    "deepnote_cell_height": 270,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     119
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 152,
    "execution_start": 1637346927803,
    "source_hash": "2bc7124d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&amp;</th>\n",
       "      <th>actor</th>\n",
       "      <th>beauti</th>\n",
       "      <th>best</th>\n",
       "      <th>citi</th>\n",
       "      <th>good</th>\n",
       "      <th>like</th>\n",
       "      <th>new</th>\n",
       "      <th>rock</th>\n",
       "      <th>roll</th>\n",
       "      <th>teacher</th>\n",
       "      <th>world</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   &  actor  beauti  best  citi  good  like  new  rock  roll  teacher  world  \\\n",
       "0  1      0       0     0     0     1     1    0     2     1        1      0   \n",
       "1  0      1       0     1     0     0     0    0     1     0        0      1   \n",
       "2  0      0       1     0     1     0     0    1     0     0        0      0   \n",
       "\n",
       "   york  \n",
       "0     0  \n",
       "1     0  \n",
       "2     1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(tokenizer= StemmerTokenizer(\"english\"))\n",
    "df = bow.fit_transform(docs)\n",
    "\n",
    "pd.DataFrame(df.toarray(), columns=bow.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d9184cbee66a498bbe0050076342f306",
    "deepnote_cell_height": 155.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Una de las cosas m치s interesantes que provee son el use de n-gramas, los cuales, en palabras simples, son conjuntos de n-palabras que se concatenan entre si y que se consideran como tokens separados. \n",
    "\n",
    "Pensemos en `Nueva York`. Cuando se tokeniza Nueva York, se generan dos tokens independientes que a simple vista no tienen relaci칩n: `Nueva` `York`.\n",
    "Al usar n-gramas (en un rango min=1,max=2) , generamos tanto `Nueva` y `York` como tambi칠n `Nueva York` como un token independiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "8aa91d98ce4d4fd1b336e730702a9832",
    "deepnote_cell_height": 301.1875,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     150.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 241,
    "execution_start": 1637346930092,
    "source_hash": "6af25c7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&amp;</th>\n",
       "      <th>&amp; roll</th>\n",
       "      <th>actor</th>\n",
       "      <th>actor world</th>\n",
       "      <th>beauti</th>\n",
       "      <th>beauti citi</th>\n",
       "      <th>best</th>\n",
       "      <th>best actor</th>\n",
       "      <th>citi</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>rock</th>\n",
       "      <th>rock &amp;</th>\n",
       "      <th>rock best</th>\n",
       "      <th>rock like</th>\n",
       "      <th>roll</th>\n",
       "      <th>teacher</th>\n",
       "      <th>teacher rock</th>\n",
       "      <th>world</th>\n",
       "      <th>york</th>\n",
       "      <th>york beauti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows 칑 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   &  & roll  actor  actor world  beauti  beauti citi  best  best actor  citi  \\\n",
       "0  1       1      0            0       0            0     0           0     0   \n",
       "1  0       0      1            1       0            0     1           1     0   \n",
       "2  0       0      0            0       1            1     0           0     1   \n",
       "\n",
       "   good  ...  rock  rock &  rock best  rock like  roll  teacher  teacher rock  \\\n",
       "0     1  ...     2       1          0          1     1        1             1   \n",
       "1     0  ...     1       0          1          0     0        0             0   \n",
       "2     0  ...     0       0          0          0     0        0             0   \n",
       "\n",
       "   world  york  york beauti  \n",
       "0      0     0            0  \n",
       "1      1     0            0  \n",
       "2      0     1            1  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(tokenizer= StemmerTokenizer(\"english\"), ngram_range=(1,2))\n",
    "df = bow.fit_transform(docs)\n",
    "\n",
    "pd.DataFrame(df.toarray(), columns=bow.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "81f6a40097954939a7854adbbe28fdc6",
    "deepnote_cell_height": 97.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "De los resultados, podemos ver que generamos vectores de conteo para cada una de las palabras que conforman el corpus.  Un punto extra que se agrega en esta obtenci칩n de frecuencias son los bigramas, que b치sicamente son el conjunto de palabras de tama침o de aparecen juntas en el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "07780e17fbf349afaba37a0aef11e2f1",
    "deepnote_cell_height": 547,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Codificando los Super{heroes, villanos}  [0.5 Puntos]\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://c.tenor.com/LkQzw7k5DV4AAAAd/anime-hacking.gif\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "Conociendo ahora que es el proceso de `bag of words`, aplique este modelo de obtenci칩n de caracteristicas de la siguiente forma en un pipeline:\n",
    "\n",
    "- Utilice el tokenizador entregado.\n",
    "- Obtenga caracteristicas de los unigramas y bigramas del texto (tal como el ejemplo).\n",
    "\n",
    "```python\n",
    "bog = CountVectorizer(tokenizer= StemmerTokenizer(),`\n",
    "                      ngram_range=(1,2) # Este punto es opcional y es para generar bigramas\n",
    "                      )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "58abb21ee53c424a96ca09d3c3f91d51",
    "deepnote_cell_height": 332.90625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Finalmente, aplique `MinMaxScaler()` sobre `atributos_de_interes` y concatene el valor obtenido con el matriz de caracteristicas obtenidas con bag of words.\n",
    "\n",
    "```python\n",
    "atributos_de_interes = ['intelligence_score', 'strength_score', 'speed_score', 'durability_score', 'power_score', 'combat_score']\n",
    "```\n",
    "\n",
    "No es necesario que obtenga un dataframe en concreto con las caracter칤sticas solicitadas. Se le recomienda generar un `ColumnTransformer()` para aplicar las transformaciones solicitadas en un pipeline.\n",
    "\n",
    "**To-Do:**\n",
    "- [ ] Obtener a traves de Bag of Words (`CountVectorizer`) caracteristicas del resumen de historia de cada personaje.\n",
    "- [ ] Aplicar `MinMaxScaler` sobre los atributos de interes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a33d10178fa84f7f8834eeaddf78f4c4",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hab칤a errores en esta celda. Las stopwords estaban en espa침ol pero las descripciones est치n en ingl칠s. Adicionalmente, se debe convertir a min칰scula. Esto ya lo hace el Stemmer, pero si eliminamos stopwords manualmente antes de aplicar el stemmer, no va a reconocer las stopwords con may칰scula. Invertimos el orden para arreglar esto.\n",
    "\n",
    "Creamos el column transformer para aplicar minmax a los atributos de inter칠s y CountVectorizer a history text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "20d7a7fe8e024e408b2016a1e4d6deef",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "id": "ay080DunHcOS"
   },
   "outputs": [],
   "source": [
    "# Fixed: Se definen las stopwords en la clase (m치s ordenado), se especifica el idioma\n",
    "# Fixed: Se aplica Stemming antes de eliminar stopwords, para no tener problemas con las may칰sculas\n",
    "class StemmerTokenizer:\n",
    "    def __init__(self, language):\n",
    "        self.ps = PorterStemmer()\n",
    "        self.stop_words = stopwords.words(language)\n",
    "    def __call__(self, doc):\n",
    "        doc_tok = word_tokenize(doc)\n",
    "        doc_tok = [self.ps.stem(t) for t in doc_tok]\n",
    "        return [t for t in doc_tok if t not in self.stop_words]\n",
    "\n",
    "atributos_de_interes = ['intelligence_score', 'strength_score', 'speed_score', 'durability_score', 'power_score', 'combat_score']\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"MinMax\", MinMaxScaler(), atributos_de_interes),\n",
    "    (\"BagOfWords\", CountVectorizer(tokenizer= StemmerTokenizer(\"english\"), ngram_range=(1,2)), \"history_text\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el column transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ct.fit_transform(df_comics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07531935406121958"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25000/features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(features[:, :6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(features[:, 6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00302915885880997"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(features[:, 6:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el minmax scaler funcion칩 ya que el m치ximo de las primeras 6 columnas (atributos_de_interes) es 1. Por otro lado, la palabra con m치s repeticiones en alguna historia se repite 1600 veces. El diccionario contiene 331914 palabras, las cuales en promedio aparecen 0.003 veces por historia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "apariciones = np.array(np.sum(features[:, 6:], axis=0))[0]\n",
    "plt.hist(apariciones[apariciones<8000], bins =2000)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7440300800809848"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(apariciones==1) / len(apariciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar c칩mo en el Bag of Words el n칰mero de veces que aparece una palabra es inversamente proporcional a la cantidad de palabras que aparece ese n칰mero de veces. De esta forma, las palabras que aparecen 1 s칩la vez (el 74%) son 10 veces m치s que las palabras que aparecen 2-3 veces, las cuales a su vez son 10 veces m치s que las palabras que aparecen 10 veces, y as칤 sucesivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9df292283e4449b2a4b5d60dcf2987c0",
    "deepnote_cell_height": 317.5,
    "deepnote_cell_type": "markdown",
    "id": "stHncQ-A-j4I",
    "owner_user_id": "d50c3174-babb-4861-9c71-7e3af66458b8"
   },
   "source": [
    "## 1.2 Dise침o de Baseline y  Primer Entrenamiento  [1 Puntos]\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://pa1.narvii.com/6374/9eaec1b7bf9157334151452a669516f9a78b954c_hq.gif\" width=\"300\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0de6d1a9fc4d400f972e7d0511ce2cf3",
    "deepnote_cell_height": 455.859375,
    "deepnote_cell_type": "markdown",
    "id": "NeMiptpQ_EWb"
   },
   "source": [
    "\n",
    "Genere un Pipeline con las caracteristicas solicitadas en la secci칩n 1.1, un selector de mejores features `SelectPercentile` con m칠trica `f_classif` y percentile=90 y un clasificador `MultinomialNB()` por defecto.\n",
    "\n",
    "Luego, separe el conjunto de datos en un conjunto de entrenamiento y prueba, donde las etiquetas estar치 dado por el atributo `alignment`. \n",
    "\n",
    "Entrene el modelo y reporte el desempe침o con un `classification_report`.  Nos recomendar칤a predecir la alineaci칩n de BatCow con este clasificador?.\n",
    "\n",
    "Finalmente, compare el modelo entrenado con un modelo Dummy estratificado y responda: 쮼l clasificador entrenado es mejor que el dummy que entrega respuestas al azar?\n",
    "\n",
    "**To-do:**\n",
    "- [ ] Realizar un pipeline con las caracteristicas solicitadas en 1.1, ejecutar holdout y aplicar un clasificador `MultinomialNB()`.\n",
    "- [ ] Entrenar el pipeline, calcular el `classification_report` asociado y comentar los resultados.\n",
    "- [ ] Entrenar un `DummyClassifier` con estrategia `statified`, calcular el `classification_report` asociado y comentar que implican los scores obtenidos en comparaci칩n con los resultados del baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "81b793138ec14f3d92713dab4e6bebb9",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "e68a1a925b9d429d93f7b3e888eb9c06",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "id": "_hHpPDooPafy"
   },
   "outputs": [],
   "source": [
    "#Creamos el pipeline. El selector de atributos s칩lo descartar치 un 10% de las columnas\n",
    "pipeline_nb = Pipeline([\n",
    "    (\"ColumnTransformer\", ct),\n",
    "    (\"FeatureSelection\",  SelectPercentile(f_classif, percentile = 90)),\n",
    "    (\"Classifier\",        MultinomialNB())\n",
    "])\n",
    "\n",
    "# Separamos el target del dataframe.\n",
    "# S칤... esto ya lo hace el columntransformer, pero es mejor ser m치s ordenados y asegurarse que el target no est치 siendo usado\n",
    "X = df_comics[df_comics.columns.difference([\"alignment\"])]\n",
    "y = df_comics.alignment\n",
    "\n",
    "# Seperamos en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, shuffle = True, random_state = 1991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las frecuencias de clase. Nos percatamos que el dataset est치 levemente desbalanceado, pero no deber칤a tener problemas para diferenciar entre Good y Bad. Vemos que los porcentajes entre train y test son muy parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Good       0.583658\n",
       "Bad        0.332685\n",
       "Neutral    0.083658\n",
       "Name: alignment, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Good       0.556420\n",
       "Bad        0.338521\n",
       "Neutral    0.105058\n",
       "Name: alignment, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts() / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nb.fit(X_train, y_train)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecimos y guardamos las predicciones en variables porque el pipeline es muy lento ya que realiza el Bag of Words en cada predicci칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = pipeline_nb.predict(X_train)\n",
    "preds_test  = pipeline_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<--M칠tricas en datos de entrenamiento-->>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.96      1.00      0.98       342\n",
      "        Good       0.97      0.99      0.98       600\n",
      "     Neutral       1.00      0.65      0.79        86\n",
      "\n",
      "    accuracy                           0.97      1028\n",
      "   macro avg       0.97      0.88      0.92      1028\n",
      "weighted avg       0.97      0.97      0.96      1028\n",
      "\n",
      "\n",
      "<<--M칠tricas en datos de validaci칩n--->>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.80      0.05      0.09        87\n",
      "        Good       0.56      0.99      0.72       143\n",
      "     Neutral       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.57       257\n",
      "   macro avg       0.45      0.35      0.27       257\n",
      "weighted avg       0.58      0.57      0.43       257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\datamining\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Miniconda3\\envs\\datamining\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Miniconda3\\envs\\datamining\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(  \"<<--M칠tricas en datos de entrenamiento-->>\")\n",
    "print(classification_report(y_train, preds_train))\n",
    "print(\"\\n<<--M칠tricas en datos de validaci칩n--->>\")\n",
    "print(classification_report(y_test, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 쯅os recomendar칤a predecir la alineaci칩n de BatCow con este clasificador?.\n",
    "\n",
    "Debido a que el clasificador tiene p칠simas m칠tricas en validaci칩n, no ser칤a recomendable usar este clasificador para predecir el alineamiento de BatCow. Como el clasificador obtuvo excelentes m칠tricas en entrenamiento pero p칠simas m칠tricas en validaci칩n, este es un cl치sico ejemplo de **sobreajuste**. El sobreajuste en clasificadores como Naive Bayes no es tan com칰n debido a lo simple de este modelo. Sin embargo, en este caso tenemos del orden de 1000 datos con **cientos de miles** de atributos, por lo que incluso los modelos m치s simples pueden sobreajustarse. M치s a칰n si consideramos que casi la totalidad de los atributos corresponden a una matriz sparse de ocurrencias de palabras, las cuales la mayor칤a (>70%) s칩lo aparecen una s칩la vez, de manera tal que el clasificado podr칤a s칩lo aprenderse estas palabras para clasificar, lo cual no es en absoluto generalizable.\n",
    "\n",
    "Para arreglar este problema, se deber칤a disminuir dr치sticamente el percentil de atributos a utilizar. Convendr칤a jugar variando entre **0.1 y 20%**. Idealmente, se deber칤an combinar distintos criterios de selecci칩n de atributos para incluir el n칰mero de veces que aparece una palabra, de manera que se descarten palabras que aparecen muy pocas veces. Tambi칠n ser칤a conveniente cambiar el clasificador a uno m치s robusto a este tipo de errores.\n",
    "\n",
    "Entrenamos un modelo Dummy y comparamos. No necesitamos recrear todo el pipeline ya que `DummyClassifier` simplemente ignora los atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_id": "f69a4b33bd9442fabbc34bcaf2c87581",
    "deepnote_cell_height": 70.796875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<--M칠tricas en datos de entrenamiento-->>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.34      0.33      0.34       342\n",
      "        Good       0.60      0.62      0.61       600\n",
      "     Neutral       0.09      0.08      0.08        86\n",
      "\n",
      "    accuracy                           0.48      1028\n",
      "   macro avg       0.34      0.34      0.34      1028\n",
      "weighted avg       0.47      0.48      0.47      1028\n",
      "\n",
      "<<--M칠tricas en datos de validaci칩n-->>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.34      0.30      0.32        87\n",
      "        Good       0.56      0.59      0.57       143\n",
      "     Neutral       0.07      0.07      0.07        27\n",
      "\n",
      "    accuracy                           0.44       257\n",
      "   macro avg       0.32      0.32      0.32       257\n",
      "weighted avg       0.43      0.44      0.44       257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy = \"stratified\", random_state = 1991)\n",
    "dummy.fit(X_train, y_train)\n",
    "print(  \"<<--M칠tricas en datos de entrenamiento-->>\")\n",
    "print(classification_report(y_train, dummy.predict(X_train)))\n",
    "\n",
    "dummy.fit(X_test, y_test)\n",
    "print(  \"<<--M칠tricas en datos de validaci칩n-->>\")\n",
    "print(classification_report(y_test, dummy.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vemos que si bien en entrenamiento el modelo Naive Bayes tiene mejores m칠tricas que el dummy lo cual es esperable, en validaci칩n sus desempe침os son similares. El modelo NB tiene mejor accuracy, mejor macro y micro average de precision y recall, pero peor macro average del f1 score. Para analizar en m치s detalle lo que est치 sucediendo en datos de validaci칩n confeccionaremos una matriz de confusi칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Dummy Classifier')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 4))\n",
    "labels = y_train.sort_values().unique()\n",
    "\n",
    "cm_nb    = confusion_matrix(y_test, preds_test)\n",
    "cm_dummy = confusion_matrix(y_test, dummy.predict(X_test))\n",
    "\n",
    "sns.heatmap(cm_nb / np.sum(cm_nb, axis =1).reshape((-1, 1)), ax = axes[0], annot = True)\n",
    "sns.heatmap(cm_dummy / np.sum(cm_dummy, axis =1).reshape((-1, 1)), ax = axes[1], annot = True)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "    \n",
    "axes[0].set_title(\"Naive Bayes Classifier\", size = 14)\n",
    "axes[1].set_title(\"Dummy Classifier\", size = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente, el clasificador Naive Bayes clasific칩 casi todos los datos fuera de muestra como la clase mayoritaria, que es `Good`. Esto probablemente ya que al no tener las 'palabras 칰nicas' en datos fuera de muestra como los de validaci칩n, el modelo se basa en otras variables que no fueron trascendentales para los datos de entrenamiento. As칤, nuestro modelo es filos칩ficamente **peor** que un clasificador dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "efd50fb630984a12a0a56ced0c73e088",
    "deepnote_cell_height": 400,
    "deepnote_cell_type": "markdown",
    "id": "pfm7I2B7_rfB"
   },
   "source": [
    "## 1.3 Busqueda del Mejor Modelo con Grid Search [4 Puntos]\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/images/70fdfeea52a8e2e4505498c230a0d2f9/tenor.gif?itemid=5134219\" width=\"250\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "69063d71deb042109162f3cb4199b231",
    "deepnote_cell_height": 859.5,
    "deepnote_cell_type": "markdown",
    "id": "14siiavzK67p"
   },
   "source": [
    "No conformes con el rendimiento obtenido en la secci칩n 1.2, el cuerpo docente les pide que realicen un **`HalvingGridSearchCV`** con diferentes par치metros para mejorar el rendimiento de la clasificaci칩n. Para esto, se le solicita que defina:\n",
    "\n",
    "- Tres clasificadores distintos en donde varie sus par치metros. Considere usar modelos cl치sicos como tambi칠n los basados en ensamblaje.\n",
    "- Modificar `n-gram` range del `CountVectorizer` probando `(1,1), (1,2) y (1,3)`. Examinar tambi칠n los otros par치metros de CountVectorizer como por ejemplo `max_df`, `min_df`, etc... ([Documentaci칩n aqu칤](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html))\n",
    "- Seleccionar las columnas que contribuyen con la mayor informaci칩n para la clasificaci칩n con `SelectPercentile` en los percentiles `[20, 40, 60, 80]` (puede usar la m칠trica que usted quiera).\n",
    "- Reporte la mejor combinaci칩n encontrada y justifique por qu칠 cree que es la mejor seg칰n el clasificador usado, la cantidad de columnas seleccionadas y los par치metros de CountVectorizer seleccionados por GridSearch.\n",
    "\n",
    "A continuaci칩n, un ejemplo de parametros para GridSearch para una b칰squeda de 3 clasificadores distintos:\n",
    "\n",
    "```python\n",
    "params = [\n",
    "       # clasificador 1 + hiperpar치metros\n",
    "       {'clf': classificator1(),\n",
    "        'clf__penalty': ['ovr'],\n",
    "       # clasificador 1 + hiperpar치metros    \n",
    "       {'clf': classificator2(),\n",
    "        'clf__n_estimators': [200]},\n",
    "       # clasificador 1 + hiperpar치metros\n",
    "       {'clf': classificator3(),\n",
    "        ...\n",
    "       }\n",
    "       ]\n",
    "```\n",
    "\n",
    "**Nota 1**: Puede ver los par치metros modificables aplicando el m칠todo get_params() sobre su pipeline. Ver la clase de GridSearch para mayor informaci칩n sobre la sint치xis de las grillas.\n",
    "\n",
    "**Nota 2**: Recuerde inicializar los clasificadores con un random state definido.\n",
    "\n",
    "**Nota 3**: Puede usar en `HalvingGridSearchCV` el par치metro `verbose=10` para ver que GridSearch le indique el estado de su ejecuci칩n.\n",
    "\n",
    "**Nota 3:** El GridSearch puede tomar tiempos de b칰squeda exorbitantes, por lo que se le recomienda no agrandar mucho el espacio de b칰squeda, dejar corriendo el c칩digo y tomarse un tecito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7c9f3de702234ddca989bf2125fab779",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Respuesta:**\n",
    "\n",
    "En un **primer approach**, se considerar치n los siguientes 3 modelos de clasificaci칩n:\n",
    "- RandomForest\n",
    "- GradientBoostingClassifier\n",
    "- SGDClassifier\n",
    "\n",
    "Para cada uno de estos modelos se probar치n distintos par치metros con la clase HalvingGridSearchCV, que nos permitir치 encontrar el mejor modelo junto con los mejores par치metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('ColumnTransformer',\n",
       "   ColumnTransformer(transformers=[('MinMax', MinMaxScaler(),\n",
       "                                    ['intelligence_score', 'strength_score',\n",
       "                                     'speed_score', 'durability_score',\n",
       "                                     'power_score', 'combat_score']),\n",
       "                                   ('BagOfWords',\n",
       "                                    CountVectorizer(ngram_range=(1, 2),\n",
       "                                                    tokenizer=<__main__.StemmerTokenizer object at 0x00000237D2B5E910>),\n",
       "                                    'history_text')])),\n",
       "  ('FeatureSelection', SelectPercentile(percentile=90)),\n",
       "  ('Model', MultinomialNB())],\n",
       " 'verbose': False,\n",
       " 'ColumnTransformer': ColumnTransformer(transformers=[('MinMax', MinMaxScaler(),\n",
       "                                  ['intelligence_score', 'strength_score',\n",
       "                                   'speed_score', 'durability_score',\n",
       "                                   'power_score', 'combat_score']),\n",
       "                                 ('BagOfWords',\n",
       "                                  CountVectorizer(ngram_range=(1, 2),\n",
       "                                                  tokenizer=<__main__.StemmerTokenizer object at 0x00000237D2B5E910>),\n",
       "                                  'history_text')]),\n",
       " 'FeatureSelection': SelectPercentile(percentile=90),\n",
       " 'Model': MultinomialNB(),\n",
       " 'ColumnTransformer__n_jobs': None,\n",
       " 'ColumnTransformer__remainder': 'drop',\n",
       " 'ColumnTransformer__sparse_threshold': 0.3,\n",
       " 'ColumnTransformer__transformer_weights': None,\n",
       " 'ColumnTransformer__transformers': [('MinMax',\n",
       "   MinMaxScaler(),\n",
       "   ['intelligence_score',\n",
       "    'strength_score',\n",
       "    'speed_score',\n",
       "    'durability_score',\n",
       "    'power_score',\n",
       "    'combat_score']),\n",
       "  ('BagOfWords',\n",
       "   CountVectorizer(ngram_range=(1, 2),\n",
       "                   tokenizer=<__main__.StemmerTokenizer object at 0x00000237D2B5E910>),\n",
       "   'history_text')],\n",
       " 'ColumnTransformer__verbose': False,\n",
       " 'ColumnTransformer__verbose_feature_names_out': True,\n",
       " 'ColumnTransformer__MinMax': MinMaxScaler(),\n",
       " 'ColumnTransformer__BagOfWords': CountVectorizer(ngram_range=(1, 2),\n",
       "                 tokenizer=<__main__.StemmerTokenizer object at 0x00000237D2B5E910>),\n",
       " 'ColumnTransformer__MinMax__clip': False,\n",
       " 'ColumnTransformer__MinMax__copy': True,\n",
       " 'ColumnTransformer__MinMax__feature_range': (0, 1),\n",
       " 'ColumnTransformer__BagOfWords__analyzer': 'word',\n",
       " 'ColumnTransformer__BagOfWords__binary': False,\n",
       " 'ColumnTransformer__BagOfWords__decode_error': 'strict',\n",
       " 'ColumnTransformer__BagOfWords__dtype': numpy.int64,\n",
       " 'ColumnTransformer__BagOfWords__encoding': 'utf-8',\n",
       " 'ColumnTransformer__BagOfWords__input': 'content',\n",
       " 'ColumnTransformer__BagOfWords__lowercase': True,\n",
       " 'ColumnTransformer__BagOfWords__max_df': 1.0,\n",
       " 'ColumnTransformer__BagOfWords__max_features': None,\n",
       " 'ColumnTransformer__BagOfWords__min_df': 1,\n",
       " 'ColumnTransformer__BagOfWords__ngram_range': (1, 2),\n",
       " 'ColumnTransformer__BagOfWords__preprocessor': None,\n",
       " 'ColumnTransformer__BagOfWords__stop_words': None,\n",
       " 'ColumnTransformer__BagOfWords__strip_accents': None,\n",
       " 'ColumnTransformer__BagOfWords__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'ColumnTransformer__BagOfWords__tokenizer': <__main__.StemmerTokenizer at 0x237d2b5e910>,\n",
       " 'ColumnTransformer__BagOfWords__vocabulary': None,\n",
       " 'FeatureSelection__percentile': 90,\n",
       " 'FeatureSelection__score_func': <function sklearn.feature_selection._univariate_selection.f_classif(X, y)>,\n",
       " 'Model__alpha': 1.0,\n",
       " 'Model__class_prior': None,\n",
       " 'Model__fit_prior': True}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"ColumnTransformer\", ct),\n",
    "        (\"FeatureSelection\",  SelectPercentile(f_classif, percentile = 90)),\n",
    "        (\"Model\",        MultinomialNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "selection_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrando en detalle en los par치metros:\n",
    "- BagOfWords__ngram_range: Permite crear n-gramas de distintos rangos, en este caso de (1,1), (1,2) y (1,3).\n",
    "- BagOfWords__max_df y min_df: Con estos par치metros es posible eliminar aquellos palabras que m치s y que menos se repiten dentro de un % de los documentos.\n",
    "- BagOfWords__max_features: Genera un corpus con las n palabras m치s frecuentes.\n",
    "- FeatureSelection__percentile: Selecciona aquellas columnas con mayor informaci칩n en los percentiles definidos.\n",
    "- Modelo: Los par치metros de los modelos van variando dependiendo de cu치l es escogido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ColumnTransformer__BagOfWords__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "  'ColumnTransformer__BagOfWords__max_features': [10000, 25000],\n",
       "  'FeatureSelection__percentile': [20, 40, 60, 80],\n",
       "  'Model': [RandomForestClassifier(random_state=1)],\n",
       "  'Model__criterion': ['gini', 'entropy'],\n",
       "  'Model__max_depth': [5, None]},\n",
       " {'ColumnTransformer__BagOfWords__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "  'ColumnTransformer__BagOfWords__max_features': [10000, 25000],\n",
       "  'FeatureSelection__percentile': [20, 40, 60, 80],\n",
       "  'Model': [GradientBoostingClassifier(random_state=1)],\n",
       "  'Model__loss': ['log_loss', 'deviance'],\n",
       "  'Model__n_estimators': [100, 200],\n",
       "  'Model__criterion': ['friedman_mse']},\n",
       " {'ColumnTransformer__BagOfWords__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "  'ColumnTransformer__BagOfWords__max_features': [10000, 25000],\n",
       "  'FeatureSelection__percentile': [20, 40, 60, 80],\n",
       "  'Model': [SGDClassifier(random_state=1)],\n",
       "  'Model__loss': ['hinge', 'log_loss'],\n",
       "  'Model__penalty': ['elasticnet'],\n",
       "  'Model__alpha': [0.001, 0.0001]}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1991)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'ColumnTransformer__BagOfWords__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "        'ColumnTransformer__BagOfWords__max_features': [10000, 25000],\n",
    "        'FeatureSelection__percentile': [20, 40, 60, 80],\n",
    "        'Model': [RandomForestClassifier(random_state=1)],\n",
    "        'Model__criterion': [\"gini\", \"entropy\"],\n",
    "        'Model__max_depth': [5, None]\n",
    "    },\n",
    "    {\n",
    "        'ColumnTransformer__BagOfWords__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "        'ColumnTransformer__BagOfWords__max_features': [10000, 25000],\n",
    "        'FeatureSelection__percentile': [20, 40, 60, 80],\n",
    "        'Model': [GradientBoostingClassifier(random_state=1)],\n",
    "        'Model__loss': [\"log_loss\", \"deviance\"],\n",
    "        'Model__n_estimators': [100, 200],\n",
    "        'Model__criterion': [\"friedman_mse\"]\n",
    "    },\n",
    "    {\n",
    "        'ColumnTransformer__BagOfWords__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "        #'ColumnTransformer__BagOfWords__max_df': [0.2, 1],\n",
    "        'ColumnTransformer__BagOfWords__max_features': [10000, 25000],\n",
    "        'FeatureSelection__percentile': [20, 40, 60, 80],\n",
    "        'Model': [SGDClassifier(random_state=1)],\n",
    "        'Model__loss': ['hinge', 'log_loss'],\n",
    "        'Model__penalty': ['elasticnet'],\n",
    "        'Model__alpha': [0.001, 0.0001]\n",
    "    },\n",
    "]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 18\n",
      "max_resources_: 1028\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 288\n",
      "n_resources: 18\n",
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "[CV 1/3; 1/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 1/3; 1/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 1/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 1/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 1/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 1/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 2/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 2/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 2/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 2/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 2/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 2/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 3/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 3/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 3/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 3/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 3/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 3/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 4/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 4/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 4/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 4/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 4/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 4/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 5/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 5/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 5/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 5/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 5/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 5/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 6/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 6/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 6/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 6/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 6/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 6/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 7/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 7/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 7/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 7/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 7/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 7/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.762) total time=   0.1s\n",
      "[CV 1/3; 8/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 8/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 8/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 8/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 8/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 8/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.762) total time=   0.1s\n",
      "[CV 1/3; 9/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 9/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 9/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 9/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 9/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 9/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.286) total time=   0.1s\n",
      "[CV 1/3; 10/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 10/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 10/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 10/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 10/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 10/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.286) total time=   0.1s\n",
      "[CV 1/3; 11/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 11/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 11/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 11/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 11/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 11/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.286) total time=   0.1s\n",
      "[CV 1/3; 12/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 12/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 12/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 12/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 12/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 12/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.286) total time=   0.1s\n",
      "[CV 1/3; 13/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 13/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 13/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 13/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 13/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 13/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 14/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 14/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 14/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 14/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 14/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 14/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 15/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 15/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 15/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 15/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 15/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 15/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 16/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 16/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 16/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 16/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 16/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 16/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 17/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 17/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 17/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 17/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 17/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 17/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 18/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 18/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 18/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 18/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 18/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 18/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 19/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 19/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 19/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 19/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 19/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 19/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 20/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 20/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 20/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 20/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 20/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 20/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 21/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 21/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 21/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 21/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 21/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 21/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 22/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 22/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 22/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 22/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 22/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 22/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 23/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 23/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 23/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 23/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 23/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 23/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 24/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 24/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 24/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 24/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 24/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 24/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 25/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 25/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 25/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 25/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 25/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 25/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 26/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 26/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 26/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 26/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 26/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 26/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 27/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 27/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 27/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 27/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 27/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 27/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 28/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 28/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 28/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 28/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 28/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 28/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 29/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 29/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 29/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 29/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 29/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 29/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 30/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 30/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 30/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 30/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 30/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 30/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 31/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 31/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 31/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 31/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 31/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 31/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 32/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 32/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 32/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 32/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 32/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 32/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 33/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 33/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 33/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 33/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 33/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 33/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 34/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 34/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 34/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 34/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 34/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 34/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 35/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 35/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 35/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 35/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 35/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 35/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 36/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 36/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 36/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 36/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 36/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 36/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 37/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 37/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 37/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 37/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 37/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 37/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 38/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 38/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 38/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 38/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 38/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 38/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 39/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 39/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 39/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 39/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 39/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 39/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 40/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 40/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 40/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 40/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 40/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 40/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 41/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 41/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 41/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 41/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 41/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 41/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 42/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 42/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 42/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 42/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 42/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 42/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 43/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 43/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 43/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 43/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 43/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 43/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 44/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 44/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 44/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 44/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 44/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 44/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 45/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 45/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 45/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 45/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 45/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 45/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 46/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 46/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 46/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 46/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 46/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 46/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 47/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 47/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 47/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 47/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 47/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 47/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 48/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 48/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 48/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 48/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 48/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 48/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 49/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 49/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 49/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 49/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 49/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 49/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 50/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 50/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 50/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 50/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 50/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 50/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 51/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 51/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 51/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 51/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 51/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 51/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 52/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 52/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 52/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 52/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 52/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 52/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 53/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 53/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 53/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 53/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 53/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 53/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 54/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 54/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 54/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 54/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 54/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 54/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 55/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 55/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 55/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 55/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 55/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 55/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.762) total time=   0.1s\n",
      "[CV 1/3; 56/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 56/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 56/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 56/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 56/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 56/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.762) total time=   0.1s\n",
      "[CV 1/3; 57/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 57/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 57/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 57/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 57/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 57/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.286) total time=   0.1s\n",
      "[CV 1/3; 58/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 58/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 58/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 58/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 58/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 58/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.286) total time=   0.1s\n",
      "[CV 1/3; 59/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 59/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 59/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 59/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 59/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 59/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.286) total time=   0.1s\n",
      "[CV 1/3; 60/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 60/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 60/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 60/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 60/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 60/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.286) total time=   0.1s\n",
      "[CV 1/3; 61/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 61/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 61/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 61/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 61/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 61/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 62/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 62/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 62/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 62/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 62/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 62/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 63/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 63/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 63/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 63/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 63/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 63/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 64/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 64/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 64/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 64/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 64/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 64/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 65/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 65/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 65/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 65/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 65/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 65/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 66/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 66/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 66/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 66/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 66/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 66/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 67/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 67/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 67/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 67/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 67/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 67/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 68/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 68/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 68/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 68/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 68/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 68/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 69/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 69/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 69/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 69/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 69/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 69/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 70/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 70/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 70/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 70/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 70/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 70/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 71/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 71/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 71/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 71/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 71/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 71/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 72/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 72/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 72/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 72/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 72/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 72/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 73/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 73/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 73/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 73/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 73/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 73/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 74/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 74/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 74/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 74/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 74/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 74/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 75/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 75/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 75/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 75/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 75/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 75/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 76/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 76/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 76/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 76/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 76/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 76/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 77/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 77/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 77/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 77/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 77/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 77/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 78/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 78/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 78/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 78/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 78/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 78/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 79/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 79/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 79/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 79/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 79/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 79/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 80/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 80/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 80/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 80/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 80/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 80/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 81/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 81/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 81/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 81/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 81/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 81/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 82/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 82/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 82/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 82/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 82/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 82/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 83/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 83/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 83/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 83/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 83/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 83/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 84/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 84/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 84/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 84/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 84/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 84/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 85/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 85/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 85/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 85/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 85/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 85/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 86/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 86/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 86/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 86/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 86/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 86/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 87/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 87/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 87/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 87/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 87/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 87/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 88/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 88/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 88/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 88/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 88/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 88/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 89/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 89/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 89/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 89/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 89/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 89/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 90/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 90/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 90/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 90/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 90/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 90/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 91/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 91/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 91/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 91/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 91/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 91/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 92/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 92/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 92/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 92/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 92/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 92/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 93/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 93/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 93/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 93/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 93/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 93/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 94/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 94/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 94/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 94/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 94/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 94/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 95/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 95/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 95/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 95/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 95/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 95/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 96/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 96/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 96/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 96/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 96/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 96/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 97/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 97/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.1s\n",
      "[CV 2/3; 97/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 97/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 97/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 97/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 98/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 98/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 98/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 98/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 98/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 98/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 99/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 99/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.1s\n",
      "[CV 2/3; 99/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 99/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 99/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 99/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 100/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 100/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 100/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 100/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 100/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 100/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 101/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 101/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 101/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 101/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 101/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 101/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 102/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 102/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 102/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 102/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 102/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 102/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 103/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 103/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 103/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 103/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 103/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 103/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 104/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 104/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 104/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 104/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 104/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 104/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 105/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 105/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 105/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 105/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 105/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 105/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 106/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 106/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 106/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 106/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 106/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 106/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 107/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 107/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 107/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 107/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 107/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 107/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 108/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 108/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 108/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 108/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 108/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 108/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 109/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 109/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 109/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 109/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 109/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 109/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 110/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 110/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 110/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 110/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 110/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 110/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 111/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 111/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 111/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 111/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 111/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 111/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 112/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 112/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 112/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 112/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 112/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 112/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 113/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 113/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 113/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 113/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 113/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 113/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 114/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 114/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 114/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 114/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 114/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 114/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 115/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 115/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 115/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 115/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 115/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 115/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 116/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 116/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 116/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 116/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 116/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 116/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 117/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 117/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 117/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 117/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 117/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 117/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 118/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 118/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 118/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 118/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 118/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 118/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 119/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 119/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 119/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 119/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 119/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 119/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 120/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 120/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 120/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 120/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 120/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 120/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 121/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 121/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 121/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 121/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 121/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 121/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 122/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 122/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 122/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 122/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 122/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 122/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 123/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 123/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 123/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 123/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 123/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 123/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 124/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 124/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 124/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 124/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 124/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 124/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 125/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 125/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 125/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 125/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 125/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 125/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 126/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 126/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 126/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 126/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 126/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 126/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 127/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 127/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 127/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 127/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 127/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 127/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 128/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 128/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 128/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 128/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 128/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 128/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 129/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 129/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 129/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 129/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 129/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 129/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 130/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 130/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 130/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 130/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 130/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 130/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 131/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 131/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 131/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 131/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 131/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 131/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 132/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 132/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 132/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 132/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 132/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 132/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 133/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 133/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 133/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 133/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 133/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 133/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 134/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 134/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 134/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 134/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 134/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 134/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 135/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 135/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 135/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 135/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 135/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 135/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 136/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 136/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 136/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 136/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 136/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 136/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 137/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 137/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 137/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 137/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 137/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 137/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 138/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 138/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.4s\n",
      "[CV 2/3; 138/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 138/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 138/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 138/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 139/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 139/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 139/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 139/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 139/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 139/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 140/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 140/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 140/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 140/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 140/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 140/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 141/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 141/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 141/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 141/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 141/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 141/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 142/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 142/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.4s\n",
      "[CV 2/3; 142/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 142/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 142/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 142/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 143/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 143/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 143/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 143/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 143/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 143/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 144/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 144/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.4s\n",
      "[CV 2/3; 144/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 144/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 144/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 144/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 145/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 145/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.1s\n",
      "[CV 2/3; 145/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 145/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 145/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 145/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 146/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 146/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 146/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 146/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 146/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 146/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 147/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 147/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.1s\n",
      "[CV 2/3; 147/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 147/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 147/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 147/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 148/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 148/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 148/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 148/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 148/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 148/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 149/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 149/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 149/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 149/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 149/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 149/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 150/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 150/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 150/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 150/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 150/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 150/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 151/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 151/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 151/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 151/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 151/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 151/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 152/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 152/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 152/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 152/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 152/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 152/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 153/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 153/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 153/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 153/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 153/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 153/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 154/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 154/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 154/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 154/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 154/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 154/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 155/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 155/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 155/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 155/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 155/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 155/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 156/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 156/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 156/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 156/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 156/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 156/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 157/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 157/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 157/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 157/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 157/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 157/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 158/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 158/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 158/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 158/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 158/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 158/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 159/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 159/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 159/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 159/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 159/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 159/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 160/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 160/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 160/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 160/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 160/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 160/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 161/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 161/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 161/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 161/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 161/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 161/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 162/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 162/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 162/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 162/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 162/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 162/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 163/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 163/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 163/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 163/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 163/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 163/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 164/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 164/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 164/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 164/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 164/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 164/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 165/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 165/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 165/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 165/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 165/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 165/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 166/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 166/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 166/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 166/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.5s\n",
      "[CV 3/3; 166/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 166/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 167/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 167/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 167/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 167/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 167/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 167/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 168/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 168/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 168/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 168/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 168/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 168/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 169/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 169/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 169/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 169/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 169/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 169/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 170/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 170/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 170/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 170/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 170/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 170/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 171/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 171/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 171/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 171/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 171/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 171/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 172/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 172/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 172/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 172/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 172/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 172/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 173/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 173/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 173/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 173/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 173/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 173/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 174/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 174/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 174/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 174/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.5s\n",
      "[CV 3/3; 174/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 174/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 175/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 175/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 175/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 175/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 175/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 175/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 176/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 176/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 176/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 176/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.5s\n",
      "[CV 3/3; 176/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 176/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 177/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 177/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 177/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 177/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 177/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 177/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 178/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 178/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 178/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 178/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 178/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 178/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 179/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 1/3; 179/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 179/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 179/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 179/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 179/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 180/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 180/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 180/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 180/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 180/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n",
      "[CV 3/3; 180/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 181/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 181/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 181/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 181/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 181/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 181/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 182/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 182/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 182/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 182/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.5s\n",
      "[CV 3/3; 182/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 182/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 183/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 183/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.2s\n",
      "[CV 2/3; 183/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 183/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 183/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 183/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 184/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 184/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 184/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 184/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.5s\n",
      "[CV 3/3; 184/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 184/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 185/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 185/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 185/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 185/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 185/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 185/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 186/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 186/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.4s\n",
      "[CV 2/3; 186/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 186/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.5s\n",
      "[CV 3/3; 186/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 186/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 187/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 187/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 187/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 187/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 187/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n",
      "[CV 3/3; 187/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 188/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 188/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.4s\n",
      "[CV 2/3; 188/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 188/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.5s\n",
      "[CV 3/3; 188/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 188/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 189/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 189/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 189/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 189/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.5s\n",
      "[CV 3/3; 189/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 189/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 190/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 190/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.4s\n",
      "[CV 2/3; 190/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 190/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.6s\n",
      "[CV 3/3; 190/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 190/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 191/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 191/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.3s\n",
      "[CV 2/3; 191/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 191/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.167) total time=   0.5s\n",
      "[CV 3/3; 191/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 191/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 192/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 192/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.250) total time=   0.4s\n",
      "[CV 2/3; 192/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 192/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.167) total time=   0.6s\n",
      "[CV 3/3; 192/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 192/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.583) total time=   0.2s\n",
      "[CV 1/3; 193/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 1/3; 193/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 193/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 193/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 193/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 193/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 194/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 194/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 194/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 194/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 194/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 194/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 195/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 195/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 195/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 195/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 195/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 195/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 196/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 196/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 196/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 196/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 196/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 196/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.762) total time=   0.1s\n",
      "[CV 1/3; 197/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 197/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.250) total time=   0.1s\n",
      "[CV 2/3; 197/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 197/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 197/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 197/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 198/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 198/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.486) total time=   0.1s\n",
      "[CV 2/3; 198/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 198/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 198/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 198/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 199/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 199/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.250) total time=   0.1s\n",
      "[CV 2/3; 199/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 199/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 199/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 199/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 200/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 200/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.486) total time=   0.1s\n",
      "[CV 2/3; 200/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 200/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 200/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 200/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 201/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 201/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 201/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 201/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 201/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 201/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 202/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 202/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 202/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 202/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 202/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 202/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 203/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 203/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 203/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 203/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 203/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 203/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 204/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 204/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 204/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 204/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 204/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 204/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 205/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 205/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 205/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 205/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 205/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 205/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 206/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 206/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 206/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 206/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 206/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 206/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 207/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 207/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 207/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 207/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 207/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 207/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 208/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 208/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 208/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 208/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 208/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 208/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 209/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 209/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 209/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 209/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 209/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 209/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 210/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 210/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 210/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 210/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 210/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 210/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 211/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 211/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 211/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 211/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 211/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 211/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 212/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 212/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 212/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 212/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 212/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 212/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 213/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 213/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 213/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 213/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 213/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 213/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 214/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 214/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.133) total time=   0.1s\n",
      "[CV 2/3; 214/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 214/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 214/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 214/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 215/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 215/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 215/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 215/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 215/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 215/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 216/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 216/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.133) total time=   0.1s\n",
      "[CV 2/3; 216/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 216/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 216/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 216/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 217/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 217/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 217/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 217/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 217/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 217/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 218/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 218/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 218/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 218/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 218/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 218/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 219/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 219/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 219/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 219/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 219/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 219/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 220/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 220/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 220/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 220/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 220/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 220/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 221/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 221/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 221/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 221/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 221/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 221/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 222/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 222/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 222/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 222/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 222/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 222/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 223/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 223/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 223/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 223/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 223/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 223/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 224/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 224/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 224/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 224/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 224/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 224/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 225/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 225/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 225/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 225/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 225/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 225/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 226/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 226/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 226/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 226/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 226/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 226/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 227/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 227/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 227/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 227/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 227/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 227/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 228/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 228/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 228/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 228/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 228/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 228/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 229/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 229/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 229/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 229/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 229/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 229/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 230/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 230/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.133) total time=   0.1s\n",
      "[CV 2/3; 230/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 230/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 230/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 230/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 231/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 231/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 231/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 231/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 231/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 231/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 232/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 232/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.133) total time=   0.1s\n",
      "[CV 2/3; 232/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 232/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 232/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 232/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 233/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 233/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 233/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 233/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 233/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 233/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 234/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 234/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 234/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 234/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 234/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 234/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 235/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 235/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 235/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 235/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 235/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 235/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 236/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 236/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 236/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 236/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 236/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 236/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 237/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 237/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 237/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 237/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 237/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 237/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 238/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 238/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 238/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 238/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 238/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 238/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 239/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 239/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 239/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 239/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 239/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 239/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 240/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 240/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 240/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 240/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 240/288] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 240/288] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 241/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 241/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 241/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 241/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 241/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 241/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 242/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 242/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 242/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 242/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 242/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 242/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 1/3; 243/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 243/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 243/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 243/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 243/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 243/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 244/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 244/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 244/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 244/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 244/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 244/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.762) total time=   0.1s\n",
      "[CV 1/3; 245/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 245/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.250) total time=   0.1s\n",
      "[CV 2/3; 245/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 245/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 245/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 245/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 246/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 246/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.486) total time=   0.1s\n",
      "[CV 2/3; 246/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 246/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 246/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 246/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 247/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 247/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.250) total time=   0.1s\n",
      "[CV 2/3; 247/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 247/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 247/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 247/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 248/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 248/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.486) total time=   0.1s\n",
      "[CV 2/3; 248/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 248/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 248/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 248/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 249/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 249/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 249/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 249/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 249/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 249/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 250/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 250/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 250/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 250/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 250/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 250/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 251/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 251/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 251/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 251/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 251/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 251/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 252/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 252/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 252/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 252/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 252/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 252/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 253/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 253/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 253/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 253/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 253/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 253/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 254/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 254/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 254/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 254/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 254/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 254/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 255/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 255/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 255/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 255/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 255/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 255/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 256/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 256/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 256/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 256/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 256/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 256/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 257/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 257/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 257/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 257/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 257/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 257/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 258/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 258/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 258/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 258/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 258/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 258/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 259/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 259/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 259/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 259/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 259/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 259/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 260/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 260/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 260/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 260/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 260/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 260/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 261/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 261/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 261/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 261/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 261/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 261/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 262/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 262/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.133) total time=   0.1s\n",
      "[CV 2/3; 262/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 262/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 262/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 262/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 263/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 263/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 263/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 263/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 263/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 263/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 264/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 264/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.133) total time=   0.1s\n",
      "[CV 2/3; 264/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 264/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 264/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 264/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 265/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 265/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 265/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 265/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 265/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 265/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 266/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 266/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 266/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 266/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 266/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 266/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 267/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 267/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 267/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 267/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 267/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 267/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 268/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 268/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 268/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 268/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 268/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 268/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 269/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 269/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 269/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 269/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 269/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 269/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 270/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 270/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 270/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 270/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 270/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 270/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 271/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 271/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 271/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 271/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 271/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 271/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 272/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 272/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 272/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 272/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 272/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 272/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 273/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 273/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 273/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 273/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 273/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 273/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 274/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 274/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 274/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 274/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 274/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 274/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 275/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 275/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 275/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 275/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 275/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 275/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 276/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 276/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.000) total time=   0.1s\n",
      "[CV 2/3; 276/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 276/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 276/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 276/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 277/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 277/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 277/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 277/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 277/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 277/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 278/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 278/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.133) total time=   0.1s\n",
      "[CV 2/3; 278/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 278/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 278/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 278/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 279/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 279/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 279/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 279/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 279/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 279/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 280/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 280/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.133) total time=   0.1s\n",
      "[CV 2/3; 280/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 280/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 280/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 280/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 281/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 281/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 281/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 281/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 281/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 281/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 282/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 282/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 282/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 282/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 282/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 282/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 283/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 283/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 283/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 283/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 283/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 283/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 284/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 284/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.244) total time=   0.1s\n",
      "[CV 2/3; 284/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 284/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 284/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 284/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 285/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 285/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 285/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 285/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 285/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 285/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 286/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 286/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 286/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 286/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 286/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 286/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 287/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 287/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.095) total time=   0.1s\n",
      "[CV 2/3; 287/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 287/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 287/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet\n",
      "[CV 3/3; 287/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=hinge, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "[CV 1/3; 288/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 288/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.111) total time=   0.1s\n",
      "[CV 2/3; 288/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:110: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 288/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 288/288] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 288/288] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.444) total time=   0.1s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 96\n",
      "n_resources: 54\n",
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "[CV 1/3; 1/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 1/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.174) total time=   0.7s\n",
      "[CV 2/3; 1/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 1/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.343) total time=   0.7s\n",
      "[CV 3/3; 1/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 1/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.286) total time=   0.6s\n",
      "[CV 1/3; 2/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 2/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   0.9s\n",
      "[CV 2/3; 2/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 2/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   0.9s\n",
      "[CV 3/3; 2/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 2/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   0.9s\n",
      "[CV 1/3; 3/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 3/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.174) total time=   0.7s\n",
      "[CV 2/3; 3/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 3/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.343) total time=   0.7s\n",
      "[CV 3/3; 3/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 3/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.286) total time=   0.6s\n",
      "[CV 1/3; 4/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 4/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   0.9s\n",
      "[CV 2/3; 4/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 4/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   0.9s\n",
      "[CV 3/3; 4/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 4/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   0.9s\n",
      "[CV 1/3; 5/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 5/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.345) total time=   0.9s\n",
      "[CV 2/3; 5/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 5/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.376) total time=   0.8s\n",
      "[CV 3/3; 5/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 5/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.463) total time=   0.7s\n",
      "[CV 1/3; 6/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 6/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.3s\n",
      "[CV 2/3; 6/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 6/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.1s\n",
      "[CV 3/3; 6/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 6/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   1.1s\n",
      "[CV 1/3; 7/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 7/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.345) total time=   0.9s\n",
      "[CV 2/3; 7/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 7/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.376) total time=   0.8s\n",
      "[CV 3/3; 7/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 7/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.463) total time=   0.8s\n",
      "[CV 1/3; 8/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 8/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.3s\n",
      "[CV 2/3; 8/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 8/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.1s\n",
      "[CV 3/3; 8/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 8/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   1.2s\n",
      "[CV 1/3; 9/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 9/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.324) total time=   1.1s\n",
      "[CV 2/3; 9/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 9/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.261) total time=   1.0s\n",
      "[CV 3/3; 9/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 9/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.240) total time=   0.9s\n",
      "[CV 1/3; 10/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 10/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.6s\n",
      "[CV 2/3; 10/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 10/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.261) total time=   1.6s\n",
      "[CV 3/3; 10/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 10/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.256) total time=   1.4s\n",
      "[CV 1/3; 11/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 11/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.324) total time=   1.0s\n",
      "[CV 2/3; 11/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 11/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.261) total time=   1.0s\n",
      "[CV 3/3; 11/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 11/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.240) total time=   0.9s\n",
      "[CV 1/3; 12/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 12/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   2.0s\n",
      "[CV 2/3; 12/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 12/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.9s\n",
      "[CV 3/3; 12/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 12/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   1.7s\n",
      "[CV 1/3; 13/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 13/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.277) total time=   1.0s\n",
      "[CV 2/3; 13/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 13/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.261) total time=   1.0s\n",
      "[CV 3/3; 13/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 13/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   0.9s\n",
      "[CV 1/3; 14/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 14/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.345) total time=   1.2s\n",
      "[CV 2/3; 14/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 14/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.235) total time=   1.2s\n",
      "[CV 3/3; 14/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 14/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.240) total time=   1.0s\n",
      "[CV 1/3; 15/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 15/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.3s\n",
      "[CV 2/3; 15/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 15/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.1s\n",
      "[CV 3/3; 15/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 15/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   1.1s\n",
      "[CV 1/3; 16/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 16/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.3s\n",
      "[CV 2/3; 16/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 16/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.1s\n",
      "[CV 3/3; 16/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 16/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   1.1s\n",
      "[CV 1/3; 17/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 17/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.317) total time=   0.6s\n",
      "[CV 2/3; 17/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 17/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.303) total time=   0.5s\n",
      "[CV 3/3; 17/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 17/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.267) total time=   0.5s\n",
      "[CV 1/3; 18/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 18/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.213) total time=   0.7s\n",
      "[CV 2/3; 18/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 18/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.278) total time=   0.7s\n",
      "[CV 3/3; 18/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 18/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.267) total time=   0.6s\n",
      "[CV 1/3; 19/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 19/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.442) total time=   0.7s\n",
      "[CV 2/3; 19/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 19/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.404) total time=   0.6s\n",
      "[CV 3/3; 19/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 19/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.593) total time=   0.5s\n",
      "[CV 1/3; 20/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 20/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.317) total time=   0.9s\n",
      "[CV 2/3; 20/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 20/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.404) total time=   0.8s\n",
      "[CV 3/3; 20/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 20/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.404) total time=   0.7s\n",
      "[CV 1/3; 21/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 21/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.442) total time=   0.7s\n",
      "[CV 2/3; 21/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 21/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.404) total time=   0.6s\n",
      "[CV 3/3; 21/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 21/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.593) total time=   0.5s\n",
      "[CV 1/3; 22/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 22/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.317) total time=   0.9s\n",
      "[CV 2/3; 22/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 22/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.404) total time=   0.8s\n",
      "[CV 3/3; 22/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 22/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.404) total time=   0.7s\n",
      "[CV 1/3; 23/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 23/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.417) total time=   0.7s\n",
      "[CV 2/3; 23/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 23/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.248) total time=   0.7s\n",
      "[CV 3/3; 23/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 23/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.232) total time=   0.6s\n",
      "[CV 1/3; 24/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 24/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.399) total time=   1.0s\n",
      "[CV 2/3; 24/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 24/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.242) total time=   1.0s\n",
      "[CV 3/3; 24/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 24/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.203) total time=   0.8s\n",
      "[CV 1/3; 25/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 25/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.417) total time=   0.8s\n",
      "[CV 2/3; 25/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 25/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.248) total time=   0.7s\n",
      "[CV 3/3; 25/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 25/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.232) total time=   0.6s\n",
      "[CV 1/3; 26/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 26/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.399) total time=   1.0s\n",
      "[CV 2/3; 26/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 26/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.242) total time=   1.0s\n",
      "[CV 3/3; 26/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 26/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.203) total time=   0.8s\n",
      "[CV 1/3; 27/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 27/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.483) total time=   0.8s\n",
      "[CV 2/3; 27/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 27/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.302) total time=   0.7s\n",
      "[CV 3/3; 27/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 27/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.203) total time=   0.6s\n",
      "[CV 1/3; 28/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 28/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.335) total time=   1.1s\n",
      "[CV 2/3; 28/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 28/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.242) total time=   1.1s\n",
      "[CV 3/3; 28/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 28/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.203) total time=   0.9s\n",
      "[CV 1/3; 29/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 29/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.483) total time=   0.8s\n",
      "[CV 2/3; 29/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 29/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.302) total time=   0.7s\n",
      "[CV 3/3; 29/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 29/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.203) total time=   0.6s\n",
      "[CV 1/3; 30/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 30/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.335) total time=   1.1s\n",
      "[CV 2/3; 30/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 30/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.242) total time=   1.1s\n",
      "[CV 3/3; 30/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 30/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.203) total time=   0.9s\n",
      "[CV 1/3; 31/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 31/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.174) total time=   0.7s\n",
      "[CV 2/3; 31/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 31/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.293) total time=   0.6s\n",
      "[CV 3/3; 31/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 31/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.463) total time=   0.6s\n",
      "[CV 1/3; 32/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 32/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   0.9s\n",
      "[CV 2/3; 32/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 32/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   0.8s\n",
      "[CV 3/3; 32/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 32/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.463) total time=   0.9s\n",
      "[CV 1/3; 33/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 33/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.174) total time=   0.7s\n",
      "[CV 2/3; 33/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 33/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.293) total time=   0.6s\n",
      "[CV 3/3; 33/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 33/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.463) total time=   0.6s\n",
      "[CV 1/3; 34/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 34/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   0.9s\n",
      "[CV 2/3; 34/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 34/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   0.9s\n",
      "[CV 3/3; 34/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 34/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.463) total time=   0.8s\n",
      "[CV 1/3; 35/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 35/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.345) total time=   0.9s\n",
      "[CV 2/3; 35/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 35/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.376) total time=   0.8s\n",
      "[CV 3/3; 35/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 35/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.286) total time=   0.7s\n",
      "[CV 1/3; 36/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 36/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.345) total time=   0.9s\n",
      "[CV 2/3; 36/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 36/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.376) total time=   0.8s\n",
      "[CV 3/3; 36/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 36/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.286) total time=   0.7s\n",
      "[CV 1/3; 37/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 37/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.9s\n",
      "[CV 2/3; 37/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 37/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.261) total time=   1.9s\n",
      "[CV 3/3; 37/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 37/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.231) total time=   1.7s\n",
      "[CV 1/3; 38/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 38/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.6s\n",
      "[CV 2/3; 38/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 38/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.261) total time=   1.7s\n",
      "[CV 3/3; 38/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 38/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.256) total time=   1.4s\n",
      "[CV 1/3; 39/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 39/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.0s\n",
      "[CV 2/3; 39/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 39/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.0s\n",
      "[CV 3/3; 39/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 39/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   0.9s\n",
      "[CV 1/3; 40/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 40/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.0s\n",
      "[CV 2/3; 40/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 40/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.0s\n",
      "[CV 3/3; 40/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 40/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   0.9s\n",
      "[CV 1/3; 41/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 41/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.174) total time=   0.8s\n",
      "[CV 2/3; 41/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 41/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.323) total time=   0.7s\n",
      "[CV 3/3; 41/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 41/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.286) total time=   0.6s\n",
      "[CV 1/3; 42/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 42/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.256) total time=   1.1s\n",
      "[CV 2/3; 42/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 42/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.343) total time=   0.9s\n",
      "[CV 3/3; 42/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 42/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.373) total time=   0.8s\n",
      "[CV 1/3; 43/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 43/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.6s\n",
      "[CV 2/3; 43/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 43/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.3s\n",
      "[CV 3/3; 43/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 43/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.378) total time=   1.2s\n",
      "[CV 1/3; 44/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 44/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.256) total time=   1.1s\n",
      "[CV 2/3; 44/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 44/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.343) total time=   0.9s\n",
      "[CV 3/3; 44/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 44/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.373) total time=   0.8s\n",
      "[CV 1/3; 45/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 45/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.6s\n",
      "[CV 2/3; 45/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 45/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.4s\n",
      "[CV 3/3; 45/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 45/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.378) total time=   1.2s\n",
      "[CV 1/3; 46/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 46/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.324) total time=   1.3s\n",
      "[CV 2/3; 46/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 46/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.261) total time=   1.2s\n",
      "[CV 3/3; 46/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 46/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.240) total time=   0.9s\n",
      "[CV 1/3; 47/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 47/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   2.0s\n",
      "[CV 2/3; 47/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 47/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.261) total time=   1.9s\n",
      "[CV 3/3; 47/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 47/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.256) total time=   1.5s\n",
      "[CV 1/3; 48/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 48/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.324) total time=   1.3s\n",
      "[CV 2/3; 48/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 48/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.261) total time=   1.2s\n",
      "[CV 3/3; 48/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 48/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.240) total time=   0.9s\n",
      "[CV 1/3; 49/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 49/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.174) total time=   0.8s\n",
      "[CV 2/3; 49/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 49/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.323) total time=   0.7s\n",
      "[CV 3/3; 49/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 49/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.286) total time=   0.6s\n",
      "[CV 1/3; 50/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 50/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   2.0s\n",
      "[CV 2/3; 50/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 50/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.261) total time=   2.0s\n",
      "[CV 3/3; 50/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 50/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.256) total time=   1.5s\n",
      "[CV 1/3; 51/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 51/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   2.4s\n",
      "[CV 2/3; 51/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 51/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.261) total time=   2.4s\n",
      "[CV 3/3; 51/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 51/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.222) total time=   1.8s\n",
      "[CV 1/3; 52/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 52/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.324) total time=   1.5s\n",
      "[CV 2/3; 52/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 52/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.261) total time=   1.4s\n",
      "[CV 3/3; 52/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 52/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   1.1s\n",
      "[CV 1/3; 53/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 53/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   2.3s\n",
      "[CV 2/3; 53/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 53/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.261) total time=   2.2s\n",
      "[CV 3/3; 53/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 53/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.222) total time=   1.7s\n",
      "[CV 1/3; 54/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 54/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.174) total time=   0.9s\n",
      "[CV 2/3; 54/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 54/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.343) total time=   0.8s\n",
      "[CV 3/3; 54/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 54/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.286) total time=   0.7s\n",
      "[CV 1/3; 55/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 55/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.2s\n",
      "[CV 2/3; 55/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 55/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.2s\n",
      "[CV 3/3; 55/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 55/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   1.1s\n",
      "[CV 1/3; 56/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 56/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.174) total time=   0.9s\n",
      "[CV 2/3; 56/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 56/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.343) total time=   0.8s\n",
      "[CV 3/3; 56/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 56/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.286) total time=   0.9s\n",
      "[CV 1/3; 57/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 57/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.6s\n",
      "[CV 2/3; 57/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 57/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.2s\n",
      "[CV 3/3; 57/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 57/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   1.4s\n",
      "[CV 1/3; 58/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 58/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.256) total time=   1.3s\n",
      "[CV 2/3; 58/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 58/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.343) total time=   1.2s\n",
      "[CV 3/3; 58/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 58/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.286) total time=   1.0s\n",
      "[CV 1/3; 59/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 59/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   2.0s\n",
      "[CV 2/3; 59/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 59/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.9s\n",
      "[CV 3/3; 59/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 59/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.286) total time=   1.6s\n",
      "[CV 1/3; 60/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 60/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.324) total time=   1.4s\n",
      "[CV 2/3; 60/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 60/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.261) total time=   1.4s\n",
      "[CV 3/3; 60/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 60/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.250) total time=   1.0s\n",
      "[CV 1/3; 61/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 61/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.335) total time=   1.1s\n",
      "[CV 2/3; 61/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 61/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.242) total time=   1.0s\n",
      "[CV 3/3; 61/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 61/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.203) total time=   0.8s\n",
      "[CV 1/3; 62/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 62/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.174) total time=   1.8s\n",
      "[CV 2/3; 62/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 62/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.261) total time=   1.9s\n",
      "[CV 3/3; 62/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 62/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.231) total time=   1.6s\n",
      "[CV 1/3; 63/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 63/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.335) total time=   1.1s\n",
      "[CV 2/3; 63/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 63/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.242) total time=   1.0s\n",
      "[CV 3/3; 63/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 63/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.203) total time=   0.8s\n",
      "[CV 1/3; 64/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 64/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.345) total time=   1.2s\n",
      "[CV 2/3; 64/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 64/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.235) total time=   1.1s\n",
      "[CV 3/3; 64/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 64/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.240) total time=   1.0s\n",
      "[CV 1/3; 65/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 65/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.317) total time=   0.6s\n",
      "[CV 2/3; 65/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 65/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.303) total time=   0.5s\n",
      "[CV 3/3; 65/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 65/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.267) total time=   0.5s\n",
      "[CV 1/3; 66/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 66/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.213) total time=   0.7s\n",
      "[CV 2/3; 66/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 66/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.278) total time=   0.6s\n",
      "[CV 3/3; 66/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 66/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.267) total time=   0.6s\n",
      "[CV 1/3; 67/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 67/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.317) total time=   0.6s\n",
      "[CV 2/3; 67/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 67/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.303) total time=   0.5s\n",
      "[CV 3/3; 67/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 67/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.267) total time=   0.5s\n",
      "[CV 1/3; 68/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 68/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.213) total time=   0.7s\n",
      "[CV 2/3; 68/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 68/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.278) total time=   0.6s\n",
      "[CV 3/3; 68/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 68/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.267) total time=   0.6s\n",
      "[CV 1/3; 69/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 69/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.483) total time=   0.8s\n",
      "[CV 2/3; 69/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 69/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.302) total time=   0.7s\n",
      "[CV 3/3; 69/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 69/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.203) total time=   0.6s\n",
      "[CV 1/3; 70/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 70/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.317) total time=   0.9s\n",
      "[CV 2/3; 70/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 70/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.404) total time=   0.8s\n",
      "[CV 3/3; 70/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 70/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.404) total time=   0.7s\n",
      "[CV 1/3; 71/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 71/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.442) total time=   0.7s\n",
      "[CV 2/3; 71/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 71/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.404) total time=   0.6s\n",
      "[CV 3/3; 71/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 71/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.593) total time=   0.5s\n",
      "[CV 1/3; 72/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 72/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.442) total time=   0.7s\n",
      "[CV 2/3; 72/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 72/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.404) total time=   0.6s\n",
      "[CV 3/3; 72/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 72/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.593) total time=   0.5s\n",
      "[CV 1/3; 73/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 73/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.483) total time=   0.8s\n",
      "[CV 2/3; 73/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 73/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.302) total time=   0.7s\n",
      "[CV 3/3; 73/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 73/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.203) total time=   0.6s\n",
      "[CV 1/3; 74/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 74/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.417) total time=   0.7s\n",
      "[CV 2/3; 74/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 74/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.248) total time=   0.7s\n",
      "[CV 3/3; 74/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 74/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.232) total time=   0.5s\n",
      "[CV 1/3; 75/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 75/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.399) total time=   1.0s\n",
      "[CV 2/3; 75/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 75/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.242) total time=   1.0s\n",
      "[CV 3/3; 75/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 75/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.203) total time=   0.8s\n",
      "[CV 1/3; 76/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 76/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.417) total time=   0.7s\n",
      "[CV 2/3; 76/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 76/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.248) total time=   0.7s\n",
      "[CV 3/3; 76/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 76/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.232) total time=   0.5s\n",
      "[CV 1/3; 77/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 77/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.399) total time=   1.0s\n",
      "[CV 2/3; 77/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 77/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.242) total time=   1.0s\n",
      "[CV 3/3; 77/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 77/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.203) total time=   0.8s\n",
      "[CV 1/3; 78/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 78/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.317) total time=   0.9s\n",
      "[CV 2/3; 78/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 78/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.404) total time=   0.8s\n",
      "[CV 3/3; 78/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 78/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.404) total time=   0.7s\n",
      "[CV 1/3; 79/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 1/3; 79/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.342) total time=   0.5s\n",
      "[CV 2/3; 79/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 2/3; 79/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 79/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 79/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.286) total time=   0.4s\n",
      "[CV 1/3; 80/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 1/3; 80/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.920, test=0.238) total time=   0.5s\n",
      "[CV 2/3; 80/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 2/3; 80/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.900, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 80/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 80/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.931, test=0.276) total time=   0.4s\n",
      "[CV 1/3; 81/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 1/3; 81/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.342) total time=   0.5s\n",
      "[CV 2/3; 81/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 2/3; 81/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 81/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 81/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.286) total time=   0.4s\n",
      "[CV 1/3; 82/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 1/3; 82/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=0.920, test=0.238) total time=   0.5s\n",
      "[CV 2/3; 82/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 2/3; 82/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=0.821, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 82/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 82/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=0.931, test=0.286) total time=   0.4s\n",
      "[CV 1/3; 83/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 1/3; 83/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.342) total time=   0.5s\n",
      "[CV 2/3; 83/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 2/3; 83/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 83/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 83/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.296) total time=   0.4s\n",
      "[CV 1/3; 84/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 1/3; 84/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.920, test=0.238) total time=   0.5s\n",
      "[CV 2/3; 84/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 2/3; 84/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.900, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 84/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 84/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.931, test=0.276) total time=   0.4s\n",
      "[CV 1/3; 85/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 1/3; 85/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=0.920, test=0.238) total time=   0.5s\n",
      "[CV 2/3; 85/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 2/3; 85/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=0.821, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 85/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5\n",
      "[CV 3/3; 85/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=5;, score=(train=0.931, test=0.286) total time=   0.4s\n",
      "[CV 1/3; 86/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 1/3; 86/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.342) total time=   0.5s\n",
      "[CV 2/3; 86/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 2/3; 86/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 86/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None\n",
      "[CV 3/3; 86/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=RandomForestClassifier(random_state=1), Model__criterion=gini, Model__max_depth=None;, score=(train=1.000, test=0.296) total time=   0.4s\n",
      "[CV 1/3; 87/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 87/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.450) total time=   0.4s\n",
      "[CV 2/3; 87/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 87/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.450) total time=   0.3s\n",
      "[CV 3/3; 87/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 87/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.276) total time=   0.3s\n",
      "[CV 1/3; 88/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 88/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.414) total time=   0.4s\n",
      "[CV 2/3; 88/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 88/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.450) total time=   0.3s\n",
      "[CV 3/3; 88/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 88/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.276) total time=   0.3s\n",
      "[CV 1/3; 89/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 89/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.414) total time=   0.4s\n",
      "[CV 2/3; 89/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 89/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.450) total time=   0.3s\n",
      "[CV 3/3; 89/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 89/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.276) total time=   0.3s\n",
      "[CV 1/3; 90/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 90/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.450) total time=   0.4s\n",
      "[CV 2/3; 90/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 90/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.450) total time=   0.3s\n",
      "[CV 3/3; 90/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 90/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.276) total time=   0.3s\n",
      "[CV 1/3; 91/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 91/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.412) total time=   0.4s\n",
      "[CV 2/3; 91/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 91/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.970, test=0.269) total time=   0.3s\n",
      "[CV 3/3; 91/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 91/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.262) total time=   0.3s\n",
      "[CV 1/3; 92/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 92/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.412) total time=   0.4s\n",
      "[CV 2/3; 92/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 92/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.970, test=0.269) total time=   0.3s\n",
      "[CV 3/3; 92/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 92/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.262) total time=   0.3s\n",
      "[CV 1/3; 93/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 1/3; 93/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.342) total time=   0.5s\n",
      "[CV 2/3; 93/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 2/3; 93/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 93/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 93/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.276) total time=   0.4s\n",
      "[CV 1/3; 94/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 1/3; 94/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.948, test=0.238) total time=   0.5s\n",
      "[CV 2/3; 94/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 2/3; 94/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.864, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 94/96] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 94/96] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.955, test=0.276) total time=   0.4s\n",
      "[CV 1/3; 95/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 1/3; 95/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.948, test=0.238) total time=   0.5s\n",
      "[CV 2/3; 95/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 2/3; 95/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.864, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 95/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5\n",
      "[CV 3/3; 95/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=5;, score=(train=0.955, test=0.276) total time=   0.4s\n",
      "[CV 1/3; 96/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 1/3; 96/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.342) total time=   0.5s\n",
      "[CV 2/3; 96/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 2/3; 96/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.187) total time=   0.4s\n",
      "[CV 3/3; 96/96] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None\n",
      "[CV 3/3; 96/96] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=RandomForestClassifier(random_state=1), Model__criterion=entropy, Model__max_depth=None;, score=(train=1.000, test=0.276) total time=   0.4s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 32\n",
      "n_resources: 162\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "[CV 1/3; 1/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 1/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.451) total time=   1.6s\n",
      "[CV 2/3; 1/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 1/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.320) total time=   2.3s\n",
      "[CV 3/3; 1/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 1/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.6s\n",
      "[CV 1/3; 2/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 2/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.451) total time=   1.6s\n",
      "[CV 2/3; 2/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 2/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.320) total time=   2.1s\n",
      "[CV 3/3; 2/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 2/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.6s\n",
      "[CV 1/3; 3/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 3/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.451) total time=   1.6s\n",
      "[CV 2/3; 3/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 3/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.320) total time=   2.1s\n",
      "[CV 3/3; 3/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 3/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.6s\n",
      "[CV 1/3; 4/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 4/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.451) total time=   1.6s\n",
      "[CV 2/3; 4/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 4/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.320) total time=   2.1s\n",
      "[CV 3/3; 4/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 4/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.6s\n",
      "[CV 1/3; 5/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 5/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.343) total time=   1.5s\n",
      "[CV 2/3; 5/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 5/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.291) total time=   2.0s\n",
      "[CV 3/3; 5/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 5/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.993, test=0.258) total time=   1.5s\n",
      "[CV 1/3; 6/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 6/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.343) total time=   1.5s\n",
      "[CV 2/3; 6/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 6/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.291) total time=   1.9s\n",
      "[CV 3/3; 6/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 6/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=0.993, test=0.258) total time=   1.5s\n",
      "[CV 1/3; 7/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 7/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.484) total time=   1.1s\n",
      "[CV 2/3; 7/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 7/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.389) total time=   1.5s\n",
      "[CV 3/3; 7/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 7/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.521) total time=   1.2s\n",
      "[CV 1/3; 8/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 8/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.484) total time=   1.1s\n",
      "[CV 2/3; 8/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 8/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.389) total time=   1.5s\n",
      "[CV 3/3; 8/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 8/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.521) total time=   1.1s\n",
      "[CV 1/3; 9/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 9/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.336) total time=   2.1s\n",
      "[CV 2/3; 9/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 9/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.330) total time=   2.7s\n",
      "[CV 3/3; 9/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 9/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.265) total time=   2.1s\n",
      "[CV 1/3; 10/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 10/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.336) total time=   2.1s\n",
      "[CV 2/3; 10/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 10/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.330) total time=   2.7s\n",
      "[CV 3/3; 10/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 10/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.265) total time=   2.1s\n",
      "[CV 1/3; 11/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 11/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.8s\n",
      "[CV 2/3; 11/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 11/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.276) total time=   2.3s\n",
      "[CV 3/3; 11/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 11/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.265) total time=   1.8s\n",
      "[CV 1/3; 12/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 12/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.343) total time=   1.8s\n",
      "[CV 2/3; 12/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 12/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.276) total time=   2.3s\n",
      "[CV 3/3; 12/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 12/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=20, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.265) total time=   1.8s\n",
      "[CV 1/3; 13/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 13/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.485) total time=   1.7s\n",
      "[CV 2/3; 13/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 13/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.311) total time=   2.2s\n",
      "[CV 3/3; 13/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 13/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.7s\n",
      "[CV 1/3; 14/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 14/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.485) total time=   1.7s\n",
      "[CV 2/3; 14/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 14/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.311) total time=   2.2s\n",
      "[CV 3/3; 14/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 14/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.7s\n",
      "[CV 1/3; 15/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 15/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.485) total time=   1.7s\n",
      "[CV 2/3; 15/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 15/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.311) total time=   2.2s\n",
      "[CV 3/3; 15/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 15/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.7s\n",
      "[CV 1/3; 16/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 16/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.485) total time=   1.7s\n",
      "[CV 2/3; 16/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 16/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.311) total time=   2.2s\n",
      "[CV 3/3; 16/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 16/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.7s\n",
      "[CV 1/3; 17/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 17/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.320) total time=   1.7s\n",
      "[CV 2/3; 17/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 17/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.291) total time=   2.2s\n",
      "[CV 3/3; 17/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 17/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.7s\n",
      "[CV 1/3; 18/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 18/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.320) total time=   1.7s\n",
      "[CV 2/3; 18/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 18/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.291) total time=   2.1s\n",
      "[CV 3/3; 18/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 18/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 2), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.8s\n",
      "[CV 1/3; 19/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 19/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.451) total time=   1.8s\n",
      "[CV 2/3; 19/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 19/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.284) total time=   2.3s\n",
      "[CV 3/3; 19/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 19/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.279) total time=   1.8s\n",
      "[CV 1/3; 20/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 20/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.451) total time=   1.8s\n",
      "[CV 2/3; 20/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 20/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.284) total time=   2.3s\n",
      "[CV 3/3; 20/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 20/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.279) total time=   1.9s\n",
      "[CV 1/3; 21/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 1/3; 21/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.451) total time=   1.8s\n",
      "[CV 2/3; 21/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 2/3; 21/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.284) total time=   2.3s\n",
      "[CV 3/3; 21/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200\n",
      "[CV 3/3; 21/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=200;, score=(train=1.000, test=0.279) total time=   1.8s\n",
      "[CV 1/3; 22/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 22/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.451) total time=   1.8s\n",
      "[CV 2/3; 22/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 22/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.284) total time=   2.3s\n",
      "[CV 3/3; 22/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 22/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=200;, score=(train=1.000, test=0.279) total time=   1.8s\n",
      "[CV 1/3; 23/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 23/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.383) total time=   1.1s\n",
      "[CV 2/3; 23/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 23/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.551) total time=   1.5s\n",
      "[CV 3/3; 23/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 23/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.458) total time=   1.2s\n",
      "[CV 1/3; 24/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 24/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.383) total time=   1.1s\n",
      "[CV 2/3; 24/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 24/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.551) total time=   1.5s\n",
      "[CV 3/3; 24/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 24/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.458) total time=   1.2s\n",
      "[CV 1/3; 25/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 25/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.393) total time=   1.1s\n",
      "[CV 2/3; 25/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 25/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.551) total time=   1.5s\n",
      "[CV 3/3; 25/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 25/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.470) total time=   1.2s\n",
      "[CV 1/3; 26/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 26/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.393) total time=   1.1s\n",
      "[CV 2/3; 26/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 26/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.551) total time=   1.5s\n",
      "[CV 3/3; 26/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 26/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.470) total time=   1.2s\n",
      "[CV 1/3; 27/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 27/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.326) total time=   1.8s\n",
      "[CV 2/3; 27/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 27/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.293) total time=   2.3s\n",
      "[CV 3/3; 27/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 27/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.8s\n",
      "[CV 1/3; 28/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 28/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.326) total time=   1.8s\n",
      "[CV 2/3; 28/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 28/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.293) total time=   2.3s\n",
      "[CV 3/3; 28/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 28/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 3), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.272) total time=   1.8s\n",
      "[CV 1/3; 29/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 29/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.451) total time=   1.5s\n",
      "[CV 2/3; 29/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 29/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.284) total time=   1.9s\n",
      "[CV 3/3; 29/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 29/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=0.993, test=0.272) total time=   1.5s\n",
      "[CV 1/3; 30/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 30/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.451) total time=   1.5s\n",
      "[CV 2/3; 30/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 30/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=1.000, test=0.284) total time=   2.0s\n",
      "[CV 3/3; 30/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 30/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=0.993, test=0.272) total time=   1.5s\n",
      "[CV 1/3; 31/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 31/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.451) total time=   1.5s\n",
      "[CV 2/3; 31/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 31/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.284) total time=   2.0s\n",
      "[CV 3/3; 31/32] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 31/32] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.993, test=0.272) total time=   1.5s\n",
      "[CV 1/3; 32/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 32/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.451) total time=   1.5s\n",
      "[CV 2/3; 32/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 32/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=1.000, test=0.284) total time=   1.9s\n",
      "[CV 3/3; 32/32] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 32/32] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.993, test=0.272) total time=   1.5s\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 11\n",
      "n_resources: 486\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "[CV 1/3; 1/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 1/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.985, test=0.387) total time=   5.2s\n",
      "[CV 2/3; 1/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 1/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.998, test=0.371) total time=   4.9s\n",
      "[CV 3/3; 1/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 1/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=60, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.995, test=0.400) total time=   5.2s\n",
      "[CV 1/3; 2/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 2/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=0.985, test=0.375) total time=   5.4s\n",
      "[CV 2/3; 2/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 2/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=0.998, test=0.395) total time=   5.0s\n",
      "[CV 3/3; 2/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 2/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=0.993, test=0.378) total time=   5.6s\n",
      "[CV 1/3; 3/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 3/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.988, test=0.359) total time=   5.8s\n",
      "[CV 2/3; 3/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 3/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.998, test=0.365) total time=   5.2s\n",
      "[CV 3/3; 3/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 3/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.995, test=0.357) total time=   5.7s\n",
      "[CV 1/3; 4/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 1/3; 4/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=0.988, test=0.359) total time=   5.6s\n",
      "[CV 2/3; 4/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 2/3; 4/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=0.998, test=0.365) total time=   5.3s\n",
      "[CV 3/3; 4/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100\n",
      "[CV 3/3; 4/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=log_loss, Model__n_estimators=100;, score=(train=0.995, test=0.357) total time=   5.6s\n",
      "[CV 1/3; 5/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 5/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.985, test=0.375) total time=   5.4s\n",
      "[CV 2/3; 5/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 5/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.998, test=0.395) total time=   5.2s\n",
      "[CV 3/3; 5/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/tmp/deep_learning/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 5/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=80, Model=GradientBoostingClassifier(random_state=1), Model__criterion=friedman_mse, Model__loss=deviance, Model__n_estimators=100;, score=(train=0.993, test=0.378) total time=   5.5s\n",
      "[CV 1/3; 6/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 6/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.473) total time=   3.8s\n",
      "[CV 2/3; 6/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 6/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.998, test=0.502) total time=   3.7s\n",
      "[CV 3/3; 6/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 6/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.972, test=0.429) total time=   3.8s\n",
      "[CV 1/3; 7/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 7/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.495) total time=   3.8s\n",
      "[CV 2/3; 7/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 7/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.998, test=0.496) total time=   3.7s\n",
      "[CV 3/3; 7/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 7/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.998, test=0.448) total time=   3.9s\n",
      "[CV 1/3; 8/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 8/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.593) total time=   3.8s\n",
      "[CV 2/3; 8/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 8/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.299, test=0.300) total time=   3.6s\n",
      "[CV 3/3; 8/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 8/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.847, test=0.393) total time=   3.7s\n",
      "[CV 1/3; 9/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 9/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.576) total time=   3.7s\n",
      "[CV 2/3; 9/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 9/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.462) total time=   3.7s\n",
      "[CV 3/3; 9/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 9/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=20, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.911, test=0.381) total time=   3.7s\n",
      "[CV 1/3; 10/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 10/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.477) total time=   3.9s\n",
      "[CV 2/3; 10/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 10/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.497) total time=   3.7s\n",
      "[CV 3/3; 10/11] START ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 10/11] END ColumnTransformer__BagOfWords__max_features=10000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.998, test=0.434) total time=   3.8s\n",
      "[CV 1/3; 11/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 1/3; 11/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.471) total time=   3.9s\n",
      "[CV 2/3; 11/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 2/3; 11/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=1.000, test=0.472) total time=   3.8s\n",
      "[CV 3/3; 11/11] START ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet\n",
      "[CV 3/3; 11/11] END ColumnTransformer__BagOfWords__max_features=25000, ColumnTransformer__BagOfWords__ngram_range=(1, 1), FeatureSelection__percentile=40, Model=SGDClassifier(random_state=1), Model__alpha=0.0001, Model__loss=log_loss, Model__penalty=elasticnet;, score=(train=0.998, test=0.448) total time=   3.8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"郊\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"郊쬪";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(cv=3,\n",
       "                    estimator=Pipeline(steps=[(&#x27;ColumnTransformer&#x27;,\n",
       "                                               ColumnTransformer(transformers=[(&#x27;MinMax&#x27;,\n",
       "                                                                                MinMaxScaler(),\n",
       "                                                                                [&#x27;intelligence_score&#x27;,\n",
       "                                                                                 &#x27;strength_score&#x27;,\n",
       "                                                                                 &#x27;speed_score&#x27;,\n",
       "                                                                                 &#x27;durability_score&#x27;,\n",
       "                                                                                 &#x27;power_score&#x27;,\n",
       "                                                                                 &#x27;combat_score&#x27;]),\n",
       "                                                                               (&#x27;BagOfWords&#x27;,\n",
       "                                                                                CountVectorizer(ngram_range=(1,\n",
       "                                                                                                             2),\n",
       "                                                                                                tokenizer=&lt;__main__.StemmerTokenizer object at 0x7f042e911930&gt;),\n",
       "                                                                                &#x27;history_...\n",
       "                                {&#x27;ColumnTransformer__BagOfWords__max_features&#x27;: [10000,\n",
       "                                                                                 25000],\n",
       "                                 &#x27;ColumnTransformer__BagOfWords__ngram_range&#x27;: [(1,\n",
       "                                                                                 1),\n",
       "                                                                                (1,\n",
       "                                                                                 2),\n",
       "                                                                                (1,\n",
       "                                                                                 3)],\n",
       "                                 &#x27;FeatureSelection__percentile&#x27;: [20, 40, 60,\n",
       "                                                                  80],\n",
       "                                 &#x27;Model&#x27;: [SGDClassifier(alpha=0.001,\n",
       "                                                         loss=&#x27;log_loss&#x27;,\n",
       "                                                         penalty=&#x27;elasticnet&#x27;,\n",
       "                                                         random_state=1)],\n",
       "                                 &#x27;Model__alpha&#x27;: [0.001, 0.0001],\n",
       "                                 &#x27;Model__loss&#x27;: [&#x27;hinge&#x27;, &#x27;log_loss&#x27;],\n",
       "                                 &#x27;Model__penalty&#x27;: [&#x27;elasticnet&#x27;]}],\n",
       "                    scoring=&#x27;f1_macro&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(cv=3,\n",
       "                    estimator=Pipeline(steps=[(&#x27;ColumnTransformer&#x27;,\n",
       "                                               ColumnTransformer(transformers=[(&#x27;MinMax&#x27;,\n",
       "                                                                                MinMaxScaler(),\n",
       "                                                                                [&#x27;intelligence_score&#x27;,\n",
       "                                                                                 &#x27;strength_score&#x27;,\n",
       "                                                                                 &#x27;speed_score&#x27;,\n",
       "                                                                                 &#x27;durability_score&#x27;,\n",
       "                                                                                 &#x27;power_score&#x27;,\n",
       "                                                                                 &#x27;combat_score&#x27;]),\n",
       "                                                                               (&#x27;BagOfWords&#x27;,\n",
       "                                                                                CountVectorizer(ngram_range=(1,\n",
       "                                                                                                             2),\n",
       "                                                                                                tokenizer=&lt;__main__.StemmerTokenizer object at 0x7f042e911930&gt;),\n",
       "                                                                                &#x27;history_...\n",
       "                                {&#x27;ColumnTransformer__BagOfWords__max_features&#x27;: [10000,\n",
       "                                                                                 25000],\n",
       "                                 &#x27;ColumnTransformer__BagOfWords__ngram_range&#x27;: [(1,\n",
       "                                                                                 1),\n",
       "                                                                                (1,\n",
       "                                                                                 2),\n",
       "                                                                                (1,\n",
       "                                                                                 3)],\n",
       "                                 &#x27;FeatureSelection__percentile&#x27;: [20, 40, 60,\n",
       "                                                                  80],\n",
       "                                 &#x27;Model&#x27;: [SGDClassifier(alpha=0.001,\n",
       "                                                         loss=&#x27;log_loss&#x27;,\n",
       "                                                         penalty=&#x27;elasticnet&#x27;,\n",
       "                                                         random_state=1)],\n",
       "                                 &#x27;Model__alpha&#x27;: [0.001, 0.0001],\n",
       "                                 &#x27;Model__loss&#x27;: [&#x27;hinge&#x27;, &#x27;log_loss&#x27;],\n",
       "                                 &#x27;Model__penalty&#x27;: [&#x27;elasticnet&#x27;]}],\n",
       "                    scoring=&#x27;f1_macro&#x27;, verbose=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;ColumnTransformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;MinMax&#x27;, MinMaxScaler(),\n",
       "                                                  [&#x27;intelligence_score&#x27;,\n",
       "                                                   &#x27;strength_score&#x27;,\n",
       "                                                   &#x27;speed_score&#x27;,\n",
       "                                                   &#x27;durability_score&#x27;,\n",
       "                                                   &#x27;power_score&#x27;,\n",
       "                                                   &#x27;combat_score&#x27;]),\n",
       "                                                 (&#x27;BagOfWords&#x27;,\n",
       "                                                  CountVectorizer(ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  tokenizer=&lt;__main__.StemmerTokenizer object at 0x7f042e911930&gt;),\n",
       "                                                  &#x27;history_text&#x27;)])),\n",
       "                (&#x27;FeatureSelection&#x27;, SelectPercentile(percentile=90)),\n",
       "                (&#x27;Model&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;MinMax&#x27;, MinMaxScaler(),\n",
       "                                 [&#x27;intelligence_score&#x27;, &#x27;strength_score&#x27;,\n",
       "                                  &#x27;speed_score&#x27;, &#x27;durability_score&#x27;,\n",
       "                                  &#x27;power_score&#x27;, &#x27;combat_score&#x27;]),\n",
       "                                (&#x27;BagOfWords&#x27;,\n",
       "                                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                                 tokenizer=&lt;__main__.StemmerTokenizer object at 0x7f042e911930&gt;),\n",
       "                                 &#x27;history_text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMax</label><div class=\"sk-toggleable__content\"><pre>[&#x27;intelligence_score&#x27;, &#x27;strength_score&#x27;, &#x27;speed_score&#x27;, &#x27;durability_score&#x27;, &#x27;power_score&#x27;, &#x27;combat_score&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BagOfWords</label><div class=\"sk-toggleable__content\"><pre>history_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2),\n",
       "                tokenizer=&lt;__main__.StemmerTokenizer object at 0x7f042e911930&gt;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectPercentile</label><div class=\"sk-toggleable__content\"><pre>SelectPercentile(percentile=90)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(cv=3,\n",
       "                    estimator=Pipeline(steps=[('ColumnTransformer',\n",
       "                                               ColumnTransformer(transformers=[('MinMax',\n",
       "                                                                                MinMaxScaler(),\n",
       "                                                                                ['intelligence_score',\n",
       "                                                                                 'strength_score',\n",
       "                                                                                 'speed_score',\n",
       "                                                                                 'durability_score',\n",
       "                                                                                 'power_score',\n",
       "                                                                                 'combat_score']),\n",
       "                                                                               ('BagOfWords',\n",
       "                                                                                CountVectorizer(ngram_range=(1,\n",
       "                                                                                                             2),\n",
       "                                                                                                tokenizer=<__main__.StemmerTokenizer object at 0x7f042e911930>),\n",
       "                                                                                'history_...\n",
       "                                {'ColumnTransformer__BagOfWords__max_features': [10000,\n",
       "                                                                                 25000],\n",
       "                                 'ColumnTransformer__BagOfWords__ngram_range': [(1,\n",
       "                                                                                 1),\n",
       "                                                                                (1,\n",
       "                                                                                 2),\n",
       "                                                                                (1,\n",
       "                                                                                 3)],\n",
       "                                 'FeatureSelection__percentile': [20, 40, 60,\n",
       "                                                                  80],\n",
       "                                 'Model': [SGDClassifier(alpha=0.001,\n",
       "                                                         loss='log_loss',\n",
       "                                                         penalty='elasticnet',\n",
       "                                                         random_state=1)],\n",
       "                                 'Model__alpha': [0.001, 0.0001],\n",
       "                                 'Model__loss': ['hinge', 'log_loss'],\n",
       "                                 'Model__penalty': ['elasticnet']}],\n",
       "                    scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgs = HalvingGridSearchCV(selection_pipeline, param_grid, scoring='f1_macro', cv=3, verbose=10)\n",
    "hgs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ColumnTransformer__BagOfWords__max_features': 25000,\n",
       " 'ColumnTransformer__BagOfWords__ngram_range': (1, 1),\n",
       " 'FeatureSelection__percentile': 40,\n",
       " 'Model': SGDClassifier(alpha=0.001, penalty='elasticnet', random_state=1),\n",
       " 'Model__alpha': 0.001,\n",
       " 'Model__loss': 'log_loss',\n",
       " 'Model__penalty': 'elasticnet'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hgs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusi칩n: \n",
      "\n",
      "[[ 47  35   5]\n",
      " [ 26 110   7]\n",
      " [  9  17   1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusi칩n: \\n\")\n",
    "print(confusion_matrix(y_test, y_pred, labels=hgs.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reporte de Clasificaci칩n: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.57      0.54      0.56        87\n",
      "        Good       0.68      0.77      0.72       143\n",
      "     Neutral       0.08      0.04      0.05        27\n",
      "\n",
      "    accuracy                           0.61       257\n",
      "   macro avg       0.44      0.45      0.44       257\n",
      "weighted avg       0.58      0.61      0.59       257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReporte de Clasificaci칩n: \\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=hgs.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "807d969e26fc4b049be6482e527b12a4",
    "deepnote_cell_height": 70.796875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Como vemos, el mejor modelo entrenado es el `SGDClassifier`, cuyos mejores par치metros fueron:\n",
    "- Loss: log_loss\n",
    "- Penalty: elasticnet\n",
    "\n",
    "Es interesante resaltar que las columnas seleccionadas se encuentran en el percentil 40 de los mejores atributos para clasificar seg칰n anova luego de seleccionar las 25000 palabras m치s frecuentes, y que el CountVectorizaer seleccionado fue de (1,1)(unigramas), es decir, se obtuvieron los mejores resultados utilizando s칩lo las palabras y no una concatenaci칩n de 칠stas para formar peque침as frases (bi-grama/tri-grama). El selector de atributos (`SelectPercentile`) s칩lo descartaba el 10% en el modelo `MultinomialNB`.\n",
    "\n",
    "Al ver resultados entregados por HalvingGridSearchCV, notamos que son mejores que los anteriores modelos entrenados, pero a칰n as칤 no son del todo prometedores. Si bien el f1-score \"macro avg\" es mayor que el modelo \"dummy\" y \"MultinomialNB()\", el accuracy es practicamente el mismo que el entregado por el modelo `MultinomialNB`. Adem치s, si vemos la matriz de confusi칩n, el modelo suele confundir la clase `Nuetral` con `Good` para la mayor칤a de datos testeados.\n",
    "\n",
    "Es por ello que en un **segundo approach** nos planteamos realizar un segundo gridsearch utilizando `TfidfTransformer` para, en lugar de utilizar las ocurrencias brutas de un token, aplicar transformaciones que nos ayudan a reducir el impacto de aquellos tokens que ocurren con mucha frecuencia y que, por esta misma raz칩n, entregan menos informaci칩n. Vemos que el modelo que mejor resultado entreg칩 fue `RandomForest` con:\n",
    "- criterion: gini\n",
    "- max_depth: None\n",
    "\n",
    "En este caso cargamos el modelo desde un `pkl`, debido a que se entren칩 de forma paralela para evitar tomarnos m치s de un tecito al entrenar. Obtenemos las predicciones y la matriz de confusi칩n y comparamos con lo obtenido en el primer approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with open('model_tfid.pkl', 'rb') as file:\n",
    "    model_tfid = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ColumnTransformer__BagOfWords__max_df': 0.2,\n",
       " 'ColumnTransformer__BagOfWords__max_features': 10000,\n",
       " 'ColumnTransformer__BagOfWords__ngram_range': (1, 1),\n",
       " 'FeatureSelection__percentile': 40,\n",
       " 'Model': RandomForestClassifier(max_features='sqrt', random_state=1),\n",
       " 'Model__criterion': 'gini',\n",
       " 'Model__max_depth': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tfid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfid = model_tfid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusi칩n RandomForest: \n",
      "\n",
      "[[ 21  65   1]\n",
      " [  9 134   0]\n",
      " [  1  25   1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusi칩n RandomForest: \\n\")\n",
    "print(confusion_matrix(y_test, y_pred_tfid, labels=model_tfid.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reporte de Clasificaci칩n RandomForest: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.68      0.24      0.36        87\n",
      "        Good       0.60      0.94      0.73       143\n",
      "     Neutral       0.50      0.04      0.07        27\n",
      "\n",
      "    accuracy                           0.61       257\n",
      "   macro avg       0.59      0.41      0.39       257\n",
      "weighted avg       0.61      0.61      0.53       257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReporte de Clasificaci칩n RandomForest: \\n\")\n",
    "print(classification_report(y_test, y_pred_tfid, target_names=model_tfid.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'RandomForest Classifier')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (10, 4))\n",
    "labels = y_train.sort_values().unique()\n",
    "\n",
    "cm_nb    = confusion_matrix(y_test, y_pred)\n",
    "cm_tfid = confusion_matrix(y_test, y_pred_tfid)\n",
    "\n",
    "sns.heatmap(cm_nb / np.sum(cm_nb, axis =1).reshape((-1, 1)), ax = axes[0], annot = True)\n",
    "sns.heatmap(cm_tfid / np.sum(cm_tfid, axis =1).reshape((-1, 1)), ax = axes[1], annot = True)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "    \n",
    "axes[0].set_title(\"SGD Classifier\", size = 14)\n",
    "axes[1].set_title(\"RandomForest Classifier\", size = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en el primer approach, las columnas seleccionadas se encuentran en el percentil 40 seg칰n annova y el CountVectorizaer seleccionado fue nuevamente de (1,1)(unigramas), pero s칩lo se utilizaron las 10000 palabras m치s frecuentes. Esta concordancia en los unigramas indicar칤a que es suficiente con el uso de palabras por s칤 solas. A칰n as칤, vemos que los resultados no mejoran, por lo que haber reducido el impacto de aquellos tokens con mucha frecuencia, en este caso, no se justifica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hiperpar치metro | Primer Modelo | Segundo Modelo |\n",
    "| --- | --- | --- |\n",
    "| Transformaci칩n de text features | --- | TfidfTransformer |\n",
    "| Clasificador | SGD | RandomForest |\n",
    "| N-grams | Unigramas | Unigramas |\n",
    "| Max features | 25000 | 10000 |\n",
    "| Percentil | 40 | 40 |\n",
    "| Total features | 10000 (3%) | 4000 (1.2%)|\n",
    "| Accuracy | 0.61 | 0.61 |\n",
    "| F1-score (macro) | 0.44 | 0.39 |\n",
    "| F1-score (micro) | 0.59 | 0.53 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, debemos elegir alguno de los dos mejores modelos logrados con cada approach. Ambos tienen el mismo accuracy, pero el modelo `SGD` tuvo mejor rendimiento seg칰n la m칠trica f1-score, lo cual se refleja notoriamente en la matriz de confusi칩n. Este estimador implementa modelos lineales con SGD y permite regularizar con una combinaci칩n de `l1` y `l2` utilizando la penalizaci칩n `elasticnet`. Este modelo tom칩 el 40% de las caracter칤sticas m치s importantes (percentil 40) y se qued칩 con las 25.000 m치s frecuentes, logrando descartar aquellas palabras que aparecen muy pocas veces y que pueden generar ruido a la hora de entrar. Por lo tanto, la selecci칩n de los par치metros y los resultados obtenidos tienen sentido, ya que se utiliza el 40% de las caracter칤sticas m치s importantes y se filtran las palabras que s칩lo aparecen una s칩la vez, evitando el ruido generalizado que el clasificador pueda aprender.\n",
    "\n",
    "Nos queda la cuesti칩n de c칩mo mejorar este modelo para aumentar las m칠tricas de la clase `Neutral`, ya que en todos los modelos obtenidos hubo problemas para clasificar esta clase. Uno de los posibles problemas de la gridsearch es que el 칩ptimo de los hiperpar치metros est칠 fuera de rango testeado. Si ese es el caso, se observar칤a siempre que el mejor modelo tiende a elegir el valor m치s extremo de alguno de los hiperpar치metros. En el caso de los `ngrams` siempre se seleccion칩 el valor m칤nimo (1,1), pero como no existen valores m치s peque침os que este no hay problema. Para el `percentil`, siempre se seleccion칩 40% que es un valor intermedio para el rango probado. Para `max_features`, en un approach se seleccion칩 25000 y en el otro 10000, por lo que el valor 칩ptimo no deber칤a estar muy lejos del rango. Tanto `max_features` como `percentil` disminuyen la cantidad de atributos a utilizar, y combinadamente producen que se seleccione un 3% y un 1.2% para el primer y segundo modelo respectivamente, por lo que s칤 se realiz칩 una podaci칩n extensiva de los atributos.\n",
    "\n",
    "Por lo tanto, s칩lo quedar칤a:\n",
    "- Ampliar el rango de los hiperpar치metros espec칤ficos del modelo\n",
    "- Probar otros modelos\n",
    "\n",
    "Como los tiempos de ejecuci칩n ya son extensos, resulta complicado realizar esto. Por ello, quiz치 convendr칤a casarnos con el `max_features = 25000` y con `percentil = 0.4`y variar otros hiperpar치metros o probar otros modelos. Si se encuentra otro modelo con mejores m칠tricas, se podr칤a evaluar reajustar el resto de hiperpar치metros. Como se trata de clasificaci칩n de texto, quiz치 lo m치s conveniente sea adentrarnos en el mundo de las **redes neuronales recurrentes**, pero para esto ya deber칤amos olvidarnos de realizar un gridsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e4fd03fac45b4d288f6c67f8bbbf8c18",
    "deepnote_cell_height": 600.15625,
    "deepnote_cell_type": "markdown",
    "id": "OmQUw2aZ_6z2"
   },
   "source": [
    "## 1.4 Predicci칩n del datos sin etiquetado  [0.5 puntos]\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://pbs.twimg.com/media/DolotxUUYAAbg7f.jpg\" width=\"350\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7f4aa54f58fd436f99c206aab5be6850",
    "deepnote_cell_height": 111.171875,
    "deepnote_cell_type": "markdown",
    "id": "Cj0ERBgTBFWN"
   },
   "source": [
    "LLego el momento de predecir \n",
    "`Vergil`, `Gorilla Girl` y `Batcow`\n",
    "\n",
    "\n",
    "**Nota:** Recuerde que pueden existir campos vacios en `history_text`, por lo que se les recomienda borrar los nan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6c3937a8832f4c48bbad30dc1b27d42d",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "cell_id": "56ba92e787044d4c9064a0bd5341842d",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>real_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>overall_score</th>\n",
       "      <th>history_text</th>\n",
       "      <th>powers_text</th>\n",
       "      <th>intelligence_score</th>\n",
       "      <th>strength_score</th>\n",
       "      <th>speed_score</th>\n",
       "      <th>durability_score</th>\n",
       "      <th>...</th>\n",
       "      <th>has_flight</th>\n",
       "      <th>has_accelerated_healing</th>\n",
       "      <th>has_weapons_master</th>\n",
       "      <th>has_intelligence</th>\n",
       "      <th>has_reflexes</th>\n",
       "      <th>has_super_speed</th>\n",
       "      <th>has_durability</th>\n",
       "      <th>has_stamina</th>\n",
       "      <th>has_agility</th>\n",
       "      <th>has_super_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>514A (Gotham)</td>\n",
       "      <td>Bruce Wayne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>He was one of the many prisoners of Indian Hil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A'dal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>As with most of the naaru, little is known of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>30</td>\n",
       "      <td>70</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Agent Zero (FOX)</td>\n",
       "      <td>David North</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>During mid-late 1973, Zero was a member of Tea...</td>\n",
       "      <td>Zero can absorb kinetic energy to further incr...</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ajax (FOX)</td>\n",
       "      <td>Francis</td>\n",
       "      <td>Francis Freeman</td>\n",
       "      <td>7</td>\n",
       "      <td>Ajax (born Francis Freeman) was a human who ga...</td>\n",
       "      <td>Ajax has claimed that the procedure to obtain ...</td>\n",
       "      <td>85</td>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>A.M.A.Z.O. (CW)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>The Anti Meta-human Adaptive Zootomic Organism...</td>\n",
       "      <td>With its metallic body, it easily overpowered ...</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows 칑 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name    real_name        full_name overall_score  \\\n",
       "1      514A (Gotham)  Bruce Wayne              NaN            10   \n",
       "17             A'dal          NaN              NaN             7   \n",
       "27  Agent Zero (FOX)  David North              NaN             6   \n",
       "31        Ajax (FOX)      Francis  Francis Freeman             7   \n",
       "46   A.M.A.Z.O. (CW)          NaN              NaN            22   \n",
       "\n",
       "                                         history_text  \\\n",
       "1   He was one of the many prisoners of Indian Hil...   \n",
       "17  As with most of the naaru, little is known of ...   \n",
       "27  During mid-late 1973, Zero was a member of Tea...   \n",
       "31  Ajax (born Francis Freeman) was a human who ga...   \n",
       "46  The Anti Meta-human Adaptive Zootomic Organism...   \n",
       "\n",
       "                                          powers_text  intelligence_score  \\\n",
       "1                                                 NaN                 100   \n",
       "17                                                NaN                  85   \n",
       "27  Zero can absorb kinetic energy to further incr...                  90   \n",
       "31  Ajax has claimed that the procedure to obtain ...                  85   \n",
       "46  With its metallic body, it easily overpowered ...                  90   \n",
       "\n",
       "    strength_score  speed_score  durability_score  ...  has_flight  \\\n",
       "1               20           30                50  ...         0.0   \n",
       "17              30           70                55  ...         0.0   \n",
       "27              10           25                25  ...         0.0   \n",
       "31              25           45                75  ...         0.0   \n",
       "46             100          100               100  ...         1.0   \n",
       "\n",
       "    has_accelerated_healing has_weapons_master has_intelligence has_reflexes  \\\n",
       "1                       0.0                0.0              0.0          1.0   \n",
       "17                      0.0                0.0              0.0          0.0   \n",
       "27                      0.0                0.0              0.0          1.0   \n",
       "31                      0.0                1.0              1.0          1.0   \n",
       "46                      0.0                1.0              0.0          0.0   \n",
       "\n",
       "   has_super_speed has_durability has_stamina  has_agility has_super_strength  \n",
       "1              0.0            1.0         0.0          0.0                1.0  \n",
       "17             0.0            0.0         0.0          0.0                0.0  \n",
       "27             0.0            0.0         0.0          1.0                0.0  \n",
       "31             0.0            1.0         0.0          0.0                1.0  \n",
       "46             0.0            0.0         0.0          0.0                0.0  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comics_no_label = df_comics_no_label.dropna(subset=['history_text'])\n",
    "df_comics_no_label = df_comics_no_label.drop_duplicates()\n",
    "df_comics_no_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vergil:  ['Good']\n",
      "Gorilla Girl:  ['Good']\n",
      "Batcow:  ['Bad']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vergil: \",hgs.predict(df_comics_no_label.loc[df_comics_no_label[\"name\"] == \"Vergil\"]))\n",
    "print(\"Gorilla Girl: \", hgs.predict(df_comics_no_label.loc[df_comics_no_label[\"name\"] == \"Gorilla Girl\"]))\n",
    "print(\"Batcow: \",hgs.predict(df_comics_no_label.loc[df_comics_no_label[\"name\"] == \"Batcow\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperarse y sospechabamos desde el inicio de este laboratorio, dentro de si misma la bativaca posee intenciones perversas. No sabemos cuales son sus intenciones, ni en qu칠 camino la llevar치 su destino, posiblemente venganza, quiz치 dominaci칩n del mundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bat-Cow was originally a cow that was found by Robin during an investigation on a slaughterhouse, which the cow was sent to. Robin wanted to keep the cow and named it Bat-Cow. Batman decided they would keep the cow. During a later investigation, Batman discovered it had mind-controlling radiation inside of it, leading him to attempt to get rid of the cow. Robin, however would not allow him to hurt the cow. After the events of Robin\\'s death, Bat-Cow and Robin\\'s dog felt a great loss in morale. Alfred Pennyworth and Nightwing had to take care of the cow. Later after Nightwing\\'s disappearance, Alfred was the only caretaker of Bat-Cow. Bat-Cow, complete with cape, spent time on a Wayne Enterprises-owned dairy farm. It was there the cow encountered the cosmic-powered \"Forever People.\" One of them seemed to gain guidance from Bat-Cow\\'s actions, namely when Bat-Cow gave one of the Forever People a scarecrow\\'s hat. Bat-Cow formed a friendship with Titus, a dog and household cat.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comics_no_label.set_index(\"name\").loc[\"Batcow\"].iloc[0][\"history_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0ee55c847633405fb2e7cfaade1fc799",
    "deepnote_cell_height": 269,
    "deepnote_cell_type": "markdown",
    "id": "Rg4ZMq8ezAH6"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/images/fb5bf7cc5a4acb91b4177672886a88ba/tenor.gif?itemid=5591338\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LCOUC4jss148",
    "GtG74Cphq56p"
   ],
   "name": "Laboratorio4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "b7ffc7ddb4f14fcd976082c27e48a913",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contenidos",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.867px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
